<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: development | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/development/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2018-12-16T10:16:31+00:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Professional Development at Artsy Engineering]]></title>
    <link href="http://artsy.github.io/blog/2016/09/22/professional-development-at-artsy-engineering/"/>
    <updated>2016-09-22T16:37:00+00:00</updated>
    <id>http://artsy.github.io/blog/2016/09/22/professional-development-at-artsy-engineering</id>
    <content type="html"><![CDATA[<p>In considering an offer to join us at Artsy, one of our newest incoming engineers asked me a great question: <em>How does the tech team do professional development?</em></p>

<p>As I thought about it, I began to realize that the answer is “a lot”! Most of our efforts evolved organically. Someone had an idea, and people rallied around it. I thought it would be useful to share, in case others are inspired by what's caught on here. Here are some of the things we do.</p>

<!-- more -->


<a name="Practices"></a>
<h1>Practices</h1>

<p>Our engineering team used to be organized along <em>practice</em> lines: web, mobile native, and platform. Following our recent <a href="http://artsy.github.io/blog/2016/03/28/artsy-engineering-organization-stack/">product reorganization</a>, we’re primarily organized in product teams, which are more deeply integrated and aligned with our business units. But practices are here to stay. They serve as horizontal channels for developers working in similar technologies to collaborate and share best practices. They each have a lively Slack channel and do standups once or twice a week.</p>

<a name="Product.teams"></a>
<h1>Product teams</h1>

<p>Within product teams, engineers take on work across the stack. Most engineers have one or more zone of expertise, but it's important for us to branch out as well. In my role as a product engineering lead, I'm responsible for helping to support this, to ensure that my team members are broadening in their skills.</p>

<a name="Collaboration"></a>
<h1>Collaboration</h1>

<p>In day-to-day work, we do a lot of pair programming and whiteboard architecture on an ad hoc basis. This happens between teams as much as within them. It's common for developers to request code reviews from members of other teams, and equally common for them to provide friendly pointers on PRs they chance upon.</p>

<a name="Lunch....n....Learn"></a>
<h1>Lunch ’n’ Learn</h1>

<p>Every Thursday, we do a Lunch-and-Learn session. Historically, we mostly showed off tech we use internally. But over about a year, most of our tech stack has been presented this way, so we also bring in engineers we know at other companies to share what they’re working on.</p>

<a name="Conferences"></a>
<h1>Conferences</h1>

<p>Devs are encouraged to attend conferences. The engineering budget covers expenses for 1 per year as an attendee, and unlimited as a presenter. Many of our engineers are experienced presenters -- and I know <a href="http://orta.io/">Orta</a> has even been a conference organizer. So there’s plenty of support for first-time presenters. Lunch ’n’ Learn sessions have served as a place to do a dry run for a presentation.</p>

<a name="Community.engagement"></a>
<h1>Community engagement</h1>

<p>Professional development isn’t just about absorbing new information, it’s also about teaching and sharing. That’s one of the best ways to truly master a topic. We encourage folks to participate in OSS, blogging (obviously), speaking, and other types of community engagement, and we’ve got really experienced people on all those fronts to support for helping make this happen. We've got team members who help maintain highly utilized community projects, but there are more low-key ways to be involved in OSS. One nice thing about being <a href="http://code.dblock.org/2015/02/09/becoming-open-source-by-default.html">open source by default</a> is that it gives us ways to contribute to OSS in the course of doing regular product development.</p>

<a name="Slack.discussions"></a>
<h1>Slack discussions</h1>

<p>We’ve got an active #tech-learning Slack channel, which people use to share and discuss articles. We also have a number of language-specific Slack channels, such as #elixir, #swift, and #scala (my personal fave).</p>

<a name="Mentorship"></a>
<h1>Mentorship</h1>

<p>Newer devs are paired with a mentor, who serves as the point person the newer dev can always talk to. Making this connection official means that it counts as a job responsibility for the mentor. For obvious reasons, mentoring benefits the mentee, but it's good to remember that it also helps mentors develop. After all, teaching someone is the best way to learn.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Paw with Per-Developer Settings]]></title>
    <link href="http://artsy.github.io/blog/2016/04/14/net-working-with-paw/"/>
    <updated>2016-04-14T12:09:00+00:00</updated>
    <id>http://artsy.github.io/blog/2016/04/14/net-working-with-paw</id>
    <content type="html"><![CDATA[<p>I am a big fan of developer tooling, as spending time upfront on improving your process can pay a lot of dividends over time. I want to talk about one in particular: <a href="https://luckymarmot.com/paw">Paw</a>. Paw is a native HTTP client with a bunch of features. I want to cover one that means that we can now <a href="https://github.com/artsy/energy/pull/192">introduce</a> <code>[AppName].paw</code> files in our mobile projects, making it easy for us to discuss networking requests.</p>

<!-- more -->


<a name="OK..what.is.Paw."></a>
<h3>OK, what is Paw?</h3>

<p>Paw is a tool that stores collections of API endpoints, along with all the metadata required to call them. We first started using Paw during the creation <a href="https://github.com/artsy/eidolon/blob/master/Kiosk/Stubbed%20Responses/Me.json">of Eidolon</a> as a way to keep track of the auction-related API calls we would need to stub for <a href="http://cocoapods.org/pods/moya">Moya</a>, an iOS networking library that required stubbed data. It made it easy for us to keep track of how all the different API routes work together, and to verify that we were doing things right.</p>

<p></div></div><a href='/images/2016-04-14-Paw/eidolon.png'><img src="/images/2016-04-14-Paw/eidolon.png" title="paw tokens" ></a><div class='meta-container'><header>&nbsp;</header></div><div class='date-container'>&nbsp;</div><div class='content-container'><div class='entry-content'></p>

<p>We used environment variables to keep track of things we wanted to change, but in using them this way we couldn't publicise our Paw files, the real versions contained secrets that should stay secret.</p>

<p><img src="/images/2016-04-14-Paw/eidolon-env.png" alt="Environments for Eidolon" /></p>

<p>The environment tooling made it easy to change the routes, users and settings easily, but were also the thing keeping us from being able to share the files in source. Because of this, we stopped using Paw to keep track of our routes as we had to ad-hoc share the file over chat.</p>

<a name="A.Second.Shot"></a>
<h3>A Second Shot</h3>

<p>This week, roughly a year and a half later, I started work on a <a href="https://github.com/artsy/energy/pull/189">large project</a> that I knew would involve using new networking APIs. So I took the time to look for ways to interpret what I was going to be working with. After exploring some alternatives, I came back to Paw, and discovered they had a <a href="https://blog.luckymarmot.com/posts/paw-23-keep-it-secret-keep-it-safe/">new feature</a>: Keychain integration. This stopped my search.</p>

<p>In our iOS projects, as they are all open source, we use <a href="https://github.com/orta/cocoapods-keys">CocoaPods-Keys</a> to ensure that our development configuration secrets are kept safe and outside of the project's source code. It stores the per-project keys inside a developer's Keychain. This means they can be accessed from inside the iOS app, but also from the <a href="/images/2016-04-14-Paw/keychain.png">developer's computer</a> via a determinate location in the Keychain app.</p>

<pre><code>~/d/i/a/energy (master) ⏛  bundle exec pod keys
Keys for Folio
 ├  ArtsyAPIClientSecret - [***********]
 ├  ArtsyAPIClientKey - [***********]
 ├  HockeyAppBetaID - [***********]
 ├  HockeyAppLiveID - [***********]
 ├  IntercomAppID - [***********]
 ├  IntercomAPIKey - [***********]
 ├  SegmentProduction - [***********]
 ├  SegmentBeta - [***********]
 └  SegmentDev - [***********]
</code></pre>

<p>This means that we can use CocoaPods-Keys with Paw in order to use the same <code>ArtsyAPIClientSecret</code> and <code>ArtsyAPIClientKey</code> environment config variables. Great. This is almost enough to make the first API call to to get an access token.</p>

<p>I re-used this idea to allow developers to have unique username and passwords. I created two more entries in Keychain, username and <a href="/images/2016-04-14-Paw/keychain-password.png">password</a>. This is something that every developer using our Paw file has to do, otherwise Paw won't know who to log you in as.</p>

<p><img src="/images/2016-04-14-Paw/keychain-username.png" alt="Keychain Email" /></p>

<p>With these all hooked up, I could set up Paw to use all of our Keychain entities:</p>

<p><img src="/images/2016-04-14-Paw/paw-adding-keychain.png" alt="Paw Adding Keychain" />
<img src="/images/2016-04-14-Paw/paw-setting-password.png" alt="Paw Setting Password" /></p>

<p>Tada! Now I can run my route, and I've got an access token to use with our API.</p>

<a name="Route.Resolving"></a>
<h3>Route Resolving</h3>

<p>Automating the route to get an access token is the first step because Paw allows you to use the output of one route inside any new route. I'll show you, then talk it through.</p>

<p></div></div><a href='/images/2016-04-14-Paw/paw-tokens.png'><img src="/images/2016-04-14-Paw/paw-tokens.png" title="paw tokens" ></a><div class='meta-container'><header>&nbsp;</header></div><div class='date-container'>&nbsp;</div><div class='content-container'><div class='entry-content'></p>

<p>I made it so that my new request ( for the route <code>api/v1/me</code>)  passes in an header of <code>X-Access-Token</code>, with the value being the <code>access_token</code> from the route we just made called <code>Auth</code>. This means that when the token expires, it will automatically re-generate a new one and we're never storing the token explicitly inside the Paw file. Our secrets stay secret, and per-developer - I don't want to know other people's passwords.</p>

<p>Once those two routes were set up, it was a matter of looking up what routes I would need and added them to the paw file for the project. I used the group system to make it easy to show / hide sections, and experimented with using environments to differentiate between staging and production servers. Not quite figured that yet.</p>

<a name="Wrap.up"></a>
<h3>Wrap up</h3>

<p>It's easier to talk about your API when any other developer can open this one file and shoot off requests at the same time as you. One of my favourite nice-touches is to be able to easily convert any request into a cURL command.</p>

<p>I am using this event as a reminder to myself that tools evolve, and maybe your first impression on a developer tool may require re-interpreting in light of software evolution.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Docker and Dusty for Development]]></title>
    <link href="http://artsy.github.io/blog/2015/12/09/docker-for-development/"/>
    <updated>2015-12-09T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/12/09/docker-for-development</id>
    <content type="html"><![CDATA[<p>When I first proposed using Docker for development, and began doing my work that way, there were some doubts.</p>

<ul>
<li>Doesn't it seem like a lot of trouble to set up Docker to get my work done?</li>
<li>Isn't it easier to use <a href="http://brew.sh/">Homebrew</a> to install the services and database servers I need?</li>
</ul>


<!-- more -->


<p>At Artsy, our main API aka Gravity uses MongoDB, Solr, Elasticsearch, and memcached. In development, we use <a href="http://mailcatcher.me/">Mailcatcher</a> so we can view emails. When a new software engineer starts, that person studies a big Getting Started document, and spends most of a day to get everything installed and configured. Not only do they need to get the software installed, figuring out all of the environment variables that need to be set up can take some time too. While we have good documentation, it is still a tedious and repetitive process that takes up the time of our new employee, and more experienced developers who need to answer questions.</p>

<p>Now that Gravity has been dockerized, getting set up consists of a one-time install of <a href="https://www.docker.com/docker-toolbox">Docker Toolbox</a> followed by running</p>

<pre><code class="bash">docker-compose build &amp;&amp; docker-compose up
</code></pre>

<p>in the root directory of the checked-out repo. Here is a simplified version of our <a href="https://docs.docker.com/compose/">docker-compose</a> setup. Because we run a web server and a delayed_job process, <code>docker-compose.yml</code> uses a <code>common.yml</code> file for shared setup:</p>

<pre><code class="yaml">gravity:
  build: .
  environment:
    MEMCACHE_SERVERS: memcached
    SOLR_URL: http://solr4:8983/solr/gravity
    MONGO_HOST: mongodb
    ELASTICSEARCH_URL: elasticsearch:9200
    SMTP_PORT: 1025
    SMTP_ADDRESS: mailcatcher
  env_file: .env
</code></pre>

<p>The <code>.env</code> file is used for secrets such as Amazon Web Services credentials we don't want to put into the git repository.</p>

<p>Our <code>docker-compose.yml</code> looks like this:</p>

<pre><code class="yaml">mongodb:
  image: mongo:2.4
  command: bash -c "rm -f /data/db/mongod.lock; mongod --smallfiles --quiet --logpath=/dev/null"
  ports: 
  - "27017:27017"

solr4:
  image: artsy/solr4

memcached:
  image: memcached

elasticsearch:
  image: artsy/elasticsearch
  ports:
  - "9200:9200"
  - "9300:9300"

web:
  extends:
    file: common.yml
    service: gravity
  command: script/rails s -b 0.0.0.0 -p 80
  ports:
  - "80:80"
  volumes:
  - .:/app
  links:
  - elasticsearch
  - mongodb
  - memcached
  - solr4
  - mailcatcher

dj:
  extends:
    file: common.yml
    service: gravity
  command: bundle exec rake jobs:work
  volumes:
  - .:/app
  links:
  - elasticsearch
  - mongodb
  - memcached
  - solr4

mailcatcher:
  image: zolweb/docker-mailcatcher
  ports:
  - "1080:1080"
</code></pre>

<p>The command for the MongoDB section removes a lock file that can remain in place sometimes when the container is killed. Do not use that in production! We mount the local directory into the container with a <code>volumes:</code> command, so that local changes are reloaded in the running containers.</p>

<p>Recently, <a href="https://github.com/ashkan18">Ashkan Nasseri</a> began to move our delayed jobs from <a href="https://github.com/collectiveidea/delayed_job_mongoid">delayed_job_mongoid</a> to <a href="http://sidekiq.org/">sidekiq</a>, which brings in Redis and another process that needs to run during development. Since we are using Docker, all we have to do is add a couple of new sections to our <code>docker-compose.yml</code> file:</p>

<pre><code class="yaml">redis:
  image: redis
  ports:
  - "6379:6379"

sidekiq:
  extends: 
    file: common.yml
    service: gravity
  command: bundle exec sidekiq
  volumes:
  - .:/app
  links:
  - elasticsearch
  - mongodb
  - memcached
  - solr4
  - redis
</code></pre>

<p>and add this line to <code>common.yml</code>:</p>

<pre><code class="yaml">REDIS_URL: redis://redis
</code></pre>

<p>The next time someone runs <code>docker-compose up</code>, this will cause a one-time download of a redis image, and then it brings up the additional sidekiq service.</p>

<p>For development which involves multiple applications in separate git repositories, we use <a href="http://dusty.gc.com/">Dusty</a>, which was created by <a href="https://gc.com/">GameChanger</a>. Some of the advantages of using Dusty include the use of NFS (which performs much better than shared volumes in VirtualBox), and a built-in nginx proxy along with modifications to your <code>/etc/hosts</code> file so that you can more easily connect to your applications.</p>

<p>With Dusty, you set up services, apps, and bundles of apps with YAML files. Here is a repo with <a href="https://github.com/gamechanger/dusty-example-specs">sample Dusty specs</a>.</p>

<p>Our MongoDB service is defined as:</p>

<pre><code class="yaml"># services/mongo2.yml
image: mongo:2.4
volumes:
- /persist/persistentMongo:/data/db
entrypoint: ["sh", "-c", "rm -f /data/db/mongod.lock; mongod --smallfiles --quiet --logpath=/dev/null"]
ports:
- "27017:27017"
</code></pre>

<p>It's not necessary to expose the ports, but in case we want to connect directly to the MongoDB instance with the <code>mongo</code> command without shelling into a container, we need it to be available on our Docker VM's IP address.</p>

<p>Our Gravity app's Dusty YAML file is:</p>

<pre><code class="yaml"># apps/gravity.yml
repo: github.com/artsy/gravity
mount: /app
build: .

depends:
  services:
  - mongo2
  - memcached
  - solr4
  - es15
  - mailcatcher
  - redis
  apps:
  - radiation

host_forwarding:
- host_name: gravity
  host_port: 80
  container_port: 80

compose:
  environment:
    RADIATION_URL: http://radiation
    MONGO_HOST: mongo2
    MEMCACHE_SERVERS: memcached
    SOLR_URL: http://solr4:8983/solr/gravity
    ELASTICSEARCH_URL: es15:9200
    SMTP_ADDRESS: mailcatcher
    SMTP_PORT: 1025
    REDIS_URL: redis://redis

commands:
  once:
  - bundle install -j 10
  - bundle exec rake db:client_applications:create_all
  - bundle exec rake db:admin:create
  always:
  - rails s -b 0.0.0.0 -p 80
</code></pre>

<p>The <code>depends:</code> configuration is similar to the <a href="https://docs.docker.com/compose/compose-file/#links">links</a> functionality of docker-compose. It makes sure that those applications (as defined in <code>apps/*.yml</code>) are running, and sets up <code>/etc/hosts</code> in the containers to allow your applications to refer to other services using their hostnames.</p>

<p>For now, Dusty doesn't have a way of sharing common setup like <code>common.yml</code> above, so there are similar configurations for our Sidekiq and Delayed Job workers.</p>

<p>Dusty uses bundles for clusters of applications that need to work together. An example bundle, for a CMS application that needs many APIs, is:</p>

<pre><code class="yaml"># apps/volt.yml
description: Volt
apps:
  - tangentApi
  - radiation
  - superposition
  - gravity
  - volt
</code></pre>

<p>We bring up that cluster of applications with</p>

<pre><code class="bash">dusty bundles activate volt
dusty up
</code></pre>

<p>As we have added new services over time, using Docker and Dusty to bring clusters of apps together has made it much easier for developers to work on projects without having to spend time on installation and configuration. Having Docker configuration in the repo also serves as good (and up-to-date) documentation of how a given application is configured and its dependencies. It is also much less resource-intensive compared to using virtual machines configured with Vagrant or another provisioning tool. All of our Docker applications and services can run in a single VM. If you are developing on Linux, you don't even need a VM!</p>

<p>We are also starting to use Docker to run integrated testing across multiple applications using Selenium. That will be covered in a future blog post.</p>
]]></content>
  </entry>
  
</feed>
