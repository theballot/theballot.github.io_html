<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ruby | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/ruby/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2018-12-16T10:16:31+00:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Apogee Technical Retrospective]]></title>
    <link href="http://artsy.github.io/blog/2018/02/06/apogee-technical-retrospective/"/>
    <updated>2018-02-06T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2018/02/06/apogee-technical-retrospective</id>
    <content type="html"><![CDATA[<p>We've previously covered <a href="/blog/2018/02/02/artsy-apogee/">what Apogee is</a> and <a href="/blog/2018/01/24/kubernetes-and-hokusai/">how it's deployed</a>, so all that's left to cover is the technology used to build it. As a refresher: Apogee is a Google Sheets Add-on we built to help our Auctions Ops team transform the data given to us by our partners into a format that our CMS can understand. This process, done manually up until now, takes a long time and is a perfect candidate for automation.</p>

<p>Apogee had some really interesting technical challenges that I enjoyed solving, and I'm excited to share some lessons I learned. So let's dive in!</p>

<!-- more -->


<p>We built a prototype as a "pure" Add-on, written only inside Google's sandbox, but that approach wouldn't work for us in production: the Add-on environment was just too difficult to work with. Google expects you to write Add-ons in their in-browser <a href="http://script.google.com">Script Editor</a> and – while whether or not that editor is <em>good</em> is a matter of preference – the environment isn't suited for collaborating or unit testing. Additionally, we could not get Add-on deploys automated, so we'd like to minimize how often we <em>have</em> to deploy.</p>

<p>So we split things up. Instead of building all Apogee's logic into an Add-on, we decided to build two pieces: a very thin Add-on and a Rails server with all the real logic.</p>

<p>(Because Apogee necessarily includes information about how our partners format their data, we decided not to open source it. Data formats are <em>probably</em> not sensitive, but that's a judgement best left up to our partners.)</p>

<a name="Apogee.Add-on"></a>
<h2>Apogee Add-on</h2>

<p>The Add-on we built is very simple, by design. Our goal was to make an Add-on that was flexible enough such that we would need to deploy it less frequently than adding new parsers.</p>

<p>Add-on responsibilities include:</p>

<ul>
<li>fetching the available parsers from the server.</li>
<li>setting up an Add-on user interface (a menu of partners, each with available parsers).</li>
<li>responding to invocations from that interface.</li>
</ul>


<p>Based on the parser selected by the user, Apogee gathers the required data from the current spreadsheet, sends it to the server for processing, and appends the results to the sheet. Pretty straightforward, you'd think.</p>

<p>Unfortunately, Google Add-ons are a bit... strange. The Add-on itself is executed in Google's datacentres (not the user's browser) and is written in <a href="https://developers.google.com/apps-script/guides/services/#basic_javascript_features">JavaScript 1.6-ish</a>. Specifically, it runs with JavaScript 1.6, plus some features from 1.7, plus some other features from 1.8, and also <a href="https://developers.google.com/apps-script/guides/services/advanced">"Google Advanced Services"</a>. The execution environment also lacks an event loop, which makes sense from Google's perspective (their servers need to know if a script execution has completed) but is still a bit unusual.</p>

<p>Rather than deal with a weird version of JavaScript, we decided to write the Add-on in <a href="https://www.typescriptlang.org">TypeScript</a> and compile down to something Google can execute. We also found <a href="https://www.npmjs.com/package/@types/google-apps-script">open source typings</a> for the Google APIs, which helped a lot. Google also provides access to certain whitelisted libraries, including <a href="https://lodash.com">Lodash</a>, which is handy.</p>

<p>Add-ons also have a somewhat complex permissions and authentication model. The <a href="https://developers.google.com/apps-script/add-ons/lifecycle">documentation</a> provided is a great illustration of why <em>complete</em> documentation is not necessarily <em>effective</em> documentation. If you already understand what you're doing, the docs are a good reference, but I found them difficult to learn from. I really like <a href="https://twitter.com/kosamari/status/852319140060823553">this explanation</a> of how to structure documentation like unit tests.</p>

<p>Permissions vary wildly depending on the execution context. For example, the <code>onOpen</code> callback is able to make network requests when the script is run as an attachment to a spreadsheet, but not when deployed. This makes it difficult to populate our menu UI, which is based off an API response. I learned to not have confidence everything was working until I saw it work end-to-end.</p>

<p>One other peculiarity of Google's API is how UI callbacks work. You could create a menu for your Add-on with the following code:</p>

<pre><code class="js">SpreadsheetApp.getUi()
  .createAddonMenu()
  .addItem('Do something', 'doSomething')
  .addToUi()

function doSomething() {
}
</code></pre>

<p>You'll notice that the callback function is specified by a <em>string</em> representing a function name (and not as a function itself, which would be more idiomatic). So, for every menu item, there must exist a corresponding function in the global scope with a corresponding name. Sadly, no parameters are passed to these callbacks, so it's impossible for a function to determine which menu item it was invoked by. Therefore, every menu item <em>must</em> have exactly <em>one</em> corresponding function. That presents a problem for an Add-on with a dynamic menu.</p>

<p>The Add-on isn't executed in a browser; we're running on Google's datacentres so let's just brute-force this. Our menu is a list of partner names, which is itself a submenu of parsers specific to that partner. That means that each menu item (and corresponding callback) can be indexed by two integers: a partner index and a operation index. So now we have a way to map from our user interface to a specific operation to perform inside <em>one</em> common menu handler.</p>

<p>Let's take a look at the actual code.</p>

<pre><code class="ts">interface Operation {
  name: string
  columns: string[]
  token: string
}

interface Partner {
  name: string
  operations: Operation[]
}

// Sets up the Add-on menu and submenus.
function setupAddon(ui: Partner[]) {
  // Reduce the ui to a list of submenus.
  const addOnMenu = ui.reduce((menu, partner, partnerIndex) =&gt; {
    // Reduce the operations list to a list of menu items.
    return menu.addSubMenu(partner.operations.reduce((memo, operation, operationIndex) =&gt; {
      return memo.addItem(operation.name, `partner${partnerIndex}Operation${operationIndex}`)
    }, SpreadsheetApp.getUi().createMenu(partner.name)))
  }, SpreadsheetApp.getUi().createAddonMenu())
  // Add the generated menu to the Add-on UI.
  addOnMenu.addToUi()
}
</code></pre>

<p>Each menu has a callback function named something like <code>partnerXOperationY</code>. Then we just generated a few thousand functions that match that format and call a shared handler <em>with</em> <code>X</code> and <code>Y</code> as parameters. The generated code looks like this:</p>

<pre><code class="js">function partner0Operation0() {
    sharedHandler(0, 0);
}
function partner0Operation1() {
    sharedHandler(0, 1);
}
function partner0Operation2() {
    sharedHandler(0, 2);
}

function sharedHandler(partnerIndex, operationIndex) {
    // TODO: Look up the appropriate parser to use.
}
</code></pre>

<p>It's not elegant, but it works. Actually, I think it does have a certain elegance, given the constraints it has to operate within.</p>

<p>So that's it! The rest of the challenges were just weird permissions issues or config problems, but the Add-on was pretty easy to build. The file generated by the TypeScript compiler is only 166 lines long, and the file with all our menu callbacks is "only" 8000 lines long. Next, let's talk about the server.</p>

<a name="Apogee.Server"></a>
<h2>Apogee Server</h2>

<p>So, Rails' philosophy is "<a href="https://en.wikipedia.org/wiki/Convention_over_configuration">convention over configuration</a>", which is pretty great as long as you know the conventions. I'd never run <code>rails new</code> before. Also, that philosophy works best when you're building <em>conventional</em> apps. Because Apogee is a bit unconventional, I was going to write Apogee in Sinatra before my colleague suggested I use Rails in <a href="http://guides.rubyonrails.org/api_app.html">API-only mode</a> instead. It seemed a bit overkill, but I also didn't want to pass up the chance to finally learn Rails.</p>

<p>The server has two endpoints:</p>

<ul>
<li><code>/ui</code> provides a list of partners and their respective parsers.</li>
<li><code>/columns</code> accepts spreadsheet columns and returns processed data (cell contents and a background colour to indicate our confidence in parsed results).</li>
</ul>


<p>We needed a way for the server to specify all its operations in a way that they could be invoked through the second endpoint. We decided to use a token-based approach: each parser has a token that can be used to invoke the parser later on. This dovetails with how I structured the parsers, too.</p>

<p>Each partner is defined by a submodule within the <code>Apogee::Parser</code> module, and each parser is defined by a class within that partner module. Let's take a look at some code.</p>

<pre><code class="rb">module Apogee
  module Parser
    module Skinner
      extend Apogee::BaseParser

      class DimensionsParser
        # Name to show in Add-on UI.
        def self.menu_name
          "Parse dimensions from Description column"
        end

        # Columns required by the `/columns` endpoint.
        def self.column_names
          %w[Description]
        end

        # Parse the columns, called from the `/columns` endpoint.
        def self.parse(columns)
          # TODO: parse the columns.
        end
      end
    end
  end
end
</code></pre>

<p>Each class within a partner is expected to have those three class methods.</p>

<p>So now that we have a defined structure for our parsers, we can use Ruby reflection to collect a list of partner modules:</p>

<pre><code class="rb">Parser.constants
  .select { |c| Parser.const_get(c).is_a? Module }
  .map do |c|
    {
      name: c,
      operations: Parser.const_get(c).public_parsers
    }
end
</code></pre>

<p>Each module also has a <code>public_parsers</code> function (inherited from <code>Apogee::BaseParser</code>) which also uses reflection:</p>

<pre><code class="rb">def public_parsers
  constants
    .select { |c| const_get(c).is_a? Class }
    .map { |c| const_get(c) }
    .map do |klass|
      {
        klass: klass.to_s,
        name: klass.menu_name,
        columns: klass.column_names,
        token: Digest::SHA256.base64digest(klass.to_s)
      }
    end
end
</code></pre>

<p>This code collects all the Ruby classes inside a module into a data structure that can be consumed by the Apogee Add-on through the <code>/ui</code> endpoint. As a bonus, the tokens are generated from the SHA256 hash of the fully-qualified parser class names. And we also avoid having to maintain a separate list of parsers that I would inevitably forget to update. Win-win.</p>

<p>All that's left to do is to lookup a parser class from a token. This is as easy as finding the class with the matching token and calling its <code>parse</code> function.</p>

<pre><code class="rb">parser = partners
  .map { |p| p[:operations] }
  .flatten
  .find { |op| op[:token] == token }
Object.const_get(parser[:klass]).parse(columns)
</code></pre>

<p>Neat!</p>

<p>This approach is <em>good</em>, but strikes me as overly object-oriented. <em>Most</em> of the parsers we're going to write are going to do the same thing: they have the same three methods and the <code>parse</code> method is basically just matching each spreadsheet cell against a regular expression. We can make a better abstraction.</p>

<p>Since the parsers are defined by the presence of a class within a partner module, we can use metaprogramming to abstract away all the common pieces and add classes to the module programmatically. The implementation is too in-depth to explain in detail here, but our partner module above could be rewritten to look like the following:</p>

<pre><code class="rb">module Apogee
  module Parser
    module Skinner
      extend Apogee::BaseParser

      add_single_column_parser(
        class_name: 'DimensionsParser',
        menu_name: 'Parse dimensions from Description column',
        column_name: 'Description',
        regex: %r{REGEX GOES HERE},
        new_columns: %w[Height Width Depth Unit]
      ) do |match|
        # TODO: Process each cell.
      end
    end
  end
end
</code></pre>

<p>I created two such methods: one that uses a single regex, and another that uses multiple regexes (for more complex needs). I also wrote a handy <code>add_all_parser</code> method which adds a sort of meta-parser, which collates the results from calling <code>parse</code> on all the <em>other</em> parsers in that module. Our Ops team just needs to click "Parse everything" and the entire spreadsheet is processed with all the parsers in seconds.</p>

<p>And of course, since all our parsers are just Ruby classes, they were easy to unit test.</p>

<p>I've done metaprogramming in other languages, and it was a lot of fun to use it in Ruby. I ran the code by my colleagues who are more experienced in Ruby than I am, and documented everything thoroughly. It's a real shame the codebase isn't open source, because I'm really proud of the approach and would love to share it with you.</p>

<a name="Apogee.Authentication"></a>
<h2>Apogee Authentication</h2>

<p>We needed to make sure that only the Add-on itself was invoking the server's endpoints. Not because the server has sensitive data – Apogee's server has no database and doesn't access any APIs – but just because it's good practice to limit access to services to only who needs them.</p>

<p>We evaluated a bunch of prospective auth strategies, including (but not limited to) the following:</p>

<ul>
<li>Whitelist Google datacentre IP addresses, block all others.</li>
<li>HTTP Basic Auth.</li>
<li>Shared secret.</li>
<li>OAuth with Artsy's API, by the user upon Add-on installation.</li>
<li>Something totally custom, or a combination of any of these.</li>
</ul>


<p>After thoughtful discussion, we decided on a solution that works for us. I'm not going to specify what we used – not because I'm that concerned about the security, but because each project and team will have their own needs. If you build a server, think carefully about what kind of authentication makes sense for you and your team.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>Apogee was a really fun project. It had a defined scope, so it was a good first Rails project for me to tackle. The Add-on helps my colleagues on the Auctions Ops team do their jobs easier, so it was intrinsically rewarding to build. And it turns out that our Gallery Partnerships team also has to import a lot of partner data into Artsy's CMS, so I'm now exploring ways Apogee can help them, too.</p>

<p>As a closing note, I want to discuss something that's been on my mind lately. I've been developing iOS apps <a href="https://ashfurrow.com/blog/5-years-of-ios/">since 2009</a>, and have a <a href="https://ashfurrow.com/books/">very intimate knowledge</a> of Objective-C, Swift, and UIKit. For a long time, I actually avoided learning new languages and frameworks because they intimidated me – starting over in a new framework, from scratch, felt like a step backward.</p>

<p>I think this is a common frame of mind, among iOS developers, among all developers. But now I regret avoiding new technology for so long. The languages and tools that I knew had become part of my identity: I was an "iOS Developer." That identity was a source of strength, but was also a limitation.</p>

<p>Developers solve problems. Sometimes those problems are best solved with iOS apps. And sometimes, they're best solved with spreadsheet plugins. After <a href="https://ashfurrow.com/blog/swift-vs-react-native-feels/">realizing</a> last year that I was limiting myself, I'm still coming to terms with how that impacts my identity. But I'll say this: if <em>I</em> can leave the safety blanket of the iOS world and build something completely new, so can you. Don't let your expertise and experience limit what you think you can build.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using OCR To Fix a Hilarious Bug]]></title>
    <link href="http://artsy.github.io/blog/2015/11/05/Using-OCR-To-Fix-A-Hilarious-Bug/"/>
    <updated>2015-11-05T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/11/05/Using-OCR-To-Fix-A-Hilarious-Bug</id>
    <content type="html"><![CDATA[<p>For a little while, we would get very strange bug reports. People would complain that artist thumbnails (viewed in several different contexts across the web and our iOS apps) would not be an image of the artist's work, but rather text, which had inexplicably become an actual JPG. This wasn't just text appearing in a <code>div</code> that should contain an <code>img</code> or something like that, these were actual JPG's that were pictures of text.</p>

<p>We would fix these as they came up, chalking the strangeness up to some relic of an old image processing pipeline, data being migrated, etc.</p>

<p>However, the reports kept coming in. This blog post is about how we diagnosed this actual bug, and how we used a simple Ruby script and OCR to help us detect and fix the existing images.</p>

<!-- more -->


<p>Here's an example of a bug report where the thumbnail for <a href="https://www.artsy.net/artist/marina-abramovic-1">Marina Abramović</a> became the text of her bio.</p>

<p><img src="/images/2015-11-12-hilarious-bug/search.png" alt="Bad Search" /></p>

<p>Here's one from our <a href="https://github.com/artsy/eigen">iOS app</a> showing that thumbnails for related artists are set to their bios as well.</p>

<p><img src="/images/2015-11-12-hilarious-bug/eigen.png" alt="Bad Related Artists" /></p>

<p>Weird right? We eventually tracked down what was going on, and it's actually perfectly summarized in <a href="https://github.com/blueimp/jQuery-File-Upload/pull/3356">this issue</a>. When someone copies text from Excel, it also generates an image of that cell or cells, and puts it into the clipboard. We immediately suspected something with <code>pasteZone</code>, and the bug was easy to reproduce - have an image in your clipboard and paste anywhere on the page.</p>

<p>We have an admin panel that allows some metadata about an artist to be edited. This includes their bio, as well as a place to upload a representative image as their 'cover thumbnail'.</p>

<p>As the issue describes, we had some text input fields, as well as a file upload form using <a href="https://github.com/blueimp/jQuery-File-Upload">Blueimp's jQuery File Upload</a>. When you don't specify a <code>pasteZone</code> it defaults to the entire document. This means that a paste event anywhere on the page will trigger that event.</p>

<p>Our editorial team was using Microsoft Excel and Word to organize some data about the artist, including bios. When ready, a team member would copy and paste the bio into the bio input text field. This would also immediately fire the event for the image upload, which now automagically became an actual picture of the text of the bio. Our API and image processing pipeline would happily accept that, leading to the incredibly bizarre bug reports.</p>

<p>My immediate fix was to specify and scope <code>pasteZone</code> (and similarly, <code>dropZone</code>) to the element the file upload widget was bound to. That would prevent the problem from happening again. Taking a quick look art some random samples of artists, it looked like potentially thousands of records might have been affected and I became interested in a programmatic way to detect these images. A manual approach would have been very cumbersome.</p>

<p>Since the images were that of text, I decided to use OCR to remove artist thumbnails that it determined had 'too much text'. This may have unset valid covers from artists that use lots of text in their work, such as <a href="https://www.artsy.net/artist/joseph-kosuth">Joseph Kosuth</a>. However, this was safe to do since we have some custom logic to fall back to an image of an iconic artwork by the artist in the case of a missing thumbnail.</p>

<p>To get OCR functionality in Ruby, I decided to use <a href="https://github.com/tesseract-ocr/tesseract">Tesseract</a>, a great OSS library. Once I had it installed, I used a <a href="https://github.com/meh/ruby-tesseract-ocr">ruby wrapper</a> to make using it easier.</p>

<p>The script eventually turned into something like:</p>

<pre><code class="ruby"># initialize and configure Tesseract
engine = Tesseract::Engine.new do |config|
  config.language  = :eng
  config.blacklist = '|'
end

# iterate over artists and pull their thumbnails
# given the URL to a publicly accessible image at img

text = engine.text_for(img)
text = text.gsub(/[^a-z ]/i, '').gsub(' ', '')
if text.length &gt; 30
  puts "Found problematic artist #{artist_doc['last']}"
  # ...
end
</code></pre>

<p>So all we do is find all the text in an image, and then remove any garbage characters or artifacts from the OCR analysis, and then use 30 as an arbitrary cutoff to determine if an image was problematic. If the image had more than 30 characters as detected by the OCR library, we wound up unsetting it from the artist.</p>

<p>The additional logic to set artist covers from their iconic artworks was already in place, and I ran this script in production, identifying and unsetting over 1000 problematic thumbnails. And we haven't gotten any new reports of this bug since then :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Splitting up a large test suite]]></title>
    <link href="http://artsy.github.io/blog/2015/09/24/splitting-up-a-large-test-suite/"/>
    <updated>2015-09-24T22:13:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/09/24/splitting-up-a-large-test-suite</id>
    <content type="html"><![CDATA[<p>A while back, we wrote about <a href="/blog/2012/10/09/how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow/">How to Run RSpec Test Suites in Parallel with Jenkins CI Build Flow</a>. A version of that still handles our largest test suite, but over time the initial division of specs became unbalanced. We ended up with some tasks that took twice as long as others. Even worse, in an attempt to rebalance task times, we ended up with awkward file patterns like <code>'spec/api/**/[a-m]*_spec.rb'</code>.</p>

<p>To keep our parallel spec tasks approximately equal in size and to support arbitrary concurrency, we've added a new <code>spec:sliced</code> task:</p>

<!-- more -->


<pre><code class="ruby">namespace :spec do
  task :set_up_spec_files do
    spec_files = Dir['spec/**/*_spec.rb']
    @spec_file_digests = Hash[spec_files.map { |f| [f, Zlib.crc32(f)] }]
  end

  RSpec::Core::RakeTask.new(:sliced, [:index, :concurrency] =&gt; :set_up_spec_files) do |t, args|
    index = args[:index].to_i
    concurrency = args[:concurrency].to_i
    t.pattern = @spec_file_digests.select { |f, d| d % concurrency == index }.keys
  end
end
</code></pre>

<p>As you can see, the <code>set_up_spec_files</code> helper task builds a hash of spec file paths and corresponding checksums. When we invoke the <code>sliced</code> task with <code>index</code> and <code>concurrency</code> values (e.g., <code>0</code> and <code>5</code>), only the spec files with checksums equal to <code>0</code> when mod-ed by <code>5</code> are run. Thus, the Jenkins build flow would look like:</p>

<pre><code class="java">parallel (
  {build("master-ci-task", tasks: "spec:sliced[0,5]")},
  {build("master-ci-task", tasks: "spec:sliced[1,5]")},
  {build("master-ci-task", tasks: "spec:sliced[2,5]")},
  {build("master-ci-task", tasks: "spec:sliced[3,5]")},
  {build("master-ci-task", tasks: "spec:sliced[4,5]")}
)
build("master-ci-succeeded")
</code></pre>

<p>Now, spec times <em>might</em> continue to be unbalanced despite files being split up approximately evenly. (For a more thorough approach based on recording spec times, see <a href="https://github.com/ArturT/knapsack">knapsack</a>.) However, this little bit of randomness was a big improvement over our previous approach, and promises to scale in a uniform manner.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Releasecop Tracks Stale Releases]]></title>
    <link href="http://artsy.github.io/blog/2015/09/01/releasecop-tracks-stale-releases/"/>
    <updated>2015-09-01T17:30:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/09/01/releasecop-tracks-stale-releases</id>
    <content type="html"><![CDATA[<p>Artsy practices a sort of <a href="http://en.wikipedia.org/wiki/Continuous_delivery">continuous delivery</a>. We keep release cycles short and the process of reviewing, testing, and deploying our software as reliable, fast, and automated as possible. (This blog has touched on these practices <a href="http://artsy.github.io/blog/categories/testing/">multiple</a> <a href="http://artsy.github.io/blog/categories/continuous-integration">times</a>.)</p>

<p>Usually, commits that have been reviewed and merged are immediately built and tested. Successfully built versions of the codebase are often automatically deployed to a staging environment. On an automated or frequent-but-manual basis, that version is deployed to a production environment. Thus, commits form a pipeline:</p>

<ul>
<li>From developers' working branches</li>
<li>To the master branch</li>
<li>Through a hopefully-successful build</li>
<li>To a staging environment</li>
<li>To production</li>
</ul>


<p>The number of apps and services we deploy has grown to <em>dozens</em> per team, so sometimes things fall through the cracks. We've been using <a href="https://github.com/joeyAghion/releasecop">Releasecop</a> for the last few months to get gentle email reminders when an environment could use a deploy.</p>

<!-- more -->


<pre><code>gem install releasecop
releasecop edit
</code></pre>

<p>This opens a <em>manifest</em> file where you can describe the sequence of git remotes and branches that make up your own release pipeline. For example:</p>

<pre><code>{
  "projects": {
    "charge": [
      { "name": "master", "git": "git@github.com:artsy/charge.git" },
      { "name": "staging", "git": "git@heroku.com:charge-staging.git" },
      { "name": "production", "git": "git@heroku.com:charge-production.git" }
    ],
    "gravity": [
      { "name": "master", "git": "git@github.com:artsy/gravity.git" },
      { "name": "master-succeeded", "git": "git@github.com:artsy/gravity.git", "branch": "master-succeeded" },
      { "name": "staging", "git": "git@github.com:artsy/gravity.git", "branch": "staging" },
      { "name": "production", "git": "git@github.com:artsy/gravity.git", "branch": "production" }
    ]
  }
}
</code></pre>

<p>The <code>charge</code> app is a typical deployment to Heroku. Work progresses from the <code>master</code> branch to a <code>charge-staging</code> app to a <code>charge-production</code> app. The <code>gravity</code> app is a more complicated, non-Heroku deployment. It updates git branches to reflect what has been merged (<code>master</code>), tested (<code>master-succeeded</code>), deployed to staging, and deployed to production.</p>

<p>Run the <code>releasecop check [app]</code> command to report the status of your apps' releases:</p>

<pre><code>$ releasecop check --all
charge...
  staging is up-to-date with master
  production is up-to-date with staging
gravity...
  master-succeeded is up-to-date with master
  staging is up-to-date with master-succeeded
  production is behind staging by:
    06ca969 2015-09-04 [config] Replace Apple Push Notification certificates that expire today. (Eloy Durán)
    171121f 2015-09-03 Admin-only API for cancelling a bid (Matthew Zikherman)
    4c5feea 2015-09-02 install mongodb client in Docker so that import rake tasks can run (Barry Hoggard)
    95347d1 2015-08-31 Update to delayed_job cookbook that works with Chef 11.10 (Joey Aghion)
2 project(s) checked. 1 environment(s) out-of-date.
</code></pre>

<p>A nightly <a href="https://jenkins-ci.org/">Jenkins</a> job emails us the results, but a cron job could work equally well.</p>

<p><a href="https://github.com/joeyAghion/releasecop">Releasecop</a> reminds us to deploy ready commits and close the loop on in-progress work. We hope you find it useful. (Pull requests are welcome!)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Generating Notifications and Personalized Emails Efficiently]]></title>
    <link href="http://artsy.github.io/blog/2014/04/24/generating-notifications-and-personalized-emails-efficiently/"/>
    <updated>2014-04-24T16:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2014/04/24/generating-notifications-and-personalized-emails-efficiently</id>
    <content type="html"><![CDATA[<p>We recently launched a new personalized email here at <a href="https://artsy.net">Artsy</a> that features content that a given user might find interesting. The goal of this post is to describe how we built a backend system that efficiently generates these e-mails for all our users. I'll talk about the first, naive implementation that had performance problems right away, and how the second implementation (currently in production) solved those issues, and whose behavior at scale is well-defined and understood. I won't go into the details of the design and layout of the mail itself and how we render the content - there are several earlier blog posts that deal with those: <a href="http://artsy.github.io/blog/2014/03/18/presenters-and-memoization-moving-logic-out-of-templates/">Presenters and Memoization</a>, <a href="http://artsy.github.io/blog/2014/03/17/ruby-helper-to-group-artworks-into-a-pinterest-style-layout-for-email/">Pinterest-style Layouts</a> and <a href="http://artsy.github.io/blog/2014/03/17/some-tips-for-email-layout-and-responsiveness/">Email Layouts and Responsiveness</a>.</p>

<p><img src="/images/2014-04-24-generating-notifications-and-personalized-emails-efficiently/percy_example.png" alt="Personalized Email Example" /></p>

<!-- more -->


<a name="Deciding.What.Content.to.Include"></a>
<h3>Deciding What Content to Include</h3>

<p>First, we had to decide what types of personalized content we wanted to feature in our mails. Users can follow artists and galleries, and so this seemed like a great place to start. We'd like to let you know about new artworks that have been uploaded by artists that you follow, as well as new shows that have been added by galleries you follow, or that are exhibiting artists you follow. Since we have location data for our galleries and our users (accomplished thru an onboarding flow, or thru geo-locating their IP address), we also want to include new shows that are opening near you. Additionally, we have a recommendations engine that recommends artworks to users based on their preferences and activity on the site, and we'd like to show some of the latest and best of such recommendations.</p>

<a name="Initial.Implementation.Ideas"></a>
<h3>Initial Implementation Ideas</h3>

<p>My first thought was to have an observer (really just some <code>after_save</code> callbacks) that will wait for data to get into a state where a user can be validly notified, and in a background task write these notifications to interested users. Here's how the base setup of our <code>Notification</code> model looked:</p>

<pre><code class="ruby">class Notification
  include Mongoid::Document
  include Mongoid::Timestamps

  belongs_to :user
  belongs_to :notifiable, polymorphic: true, inverse_of: :notifications
end
</code></pre>

<p>It's a simple join of the user and the <code>notifiable</code> (the object/action that a user is being notified about).</p>

<p>Then, a specific notification (such as one for published artworks by artists you follow) can inherit from this and look like:</p>

<pre><code class="ruby">class PublishedArtworkNotification &lt; Notification
  def self.notify!(artwork_id)
    user_ids = FollowArtist.where(artist: artwork.artist).pluck("user_id")
    user_ids.each_slice(100) do |uids|
      PublishedArtworkNotification.delay(queue: :any, priority: 6).create_for_users(artwork_id, uids)
    end
  end

  def self.create_for_users(artwork, uids)
    uids.each do |uid|
      PublishedArtworkNotification.create!(user_id: uid, notifiable_id: artwork_id, notifiable_type: 'Artwork')
    end
  end
end
</code></pre>

<p>Then, the <code>after_save</code> hook on an <code>Artwork</code> model is:</p>

<pre><code class="ruby">def delay_notify
  PublishedArtworkNotification.delay(queue: :any).notify!(self.id) if self.published_changed? &amp;&amp; self.published
end
</code></pre>

<p>So, when an artwork is published, we run <code>notify!</code> in the background for the respective notification. That method will pull all interested users (via following the artist), and then spawn off more background processes to write the notifications in batches of 100. We batched these writes to avoid any one background process taking too long (an artist such as <a href="https://artsy.net/artist/andy-warhol">Andy Warhol</a> has around twelve thousand followers currently), and also ran them at a lower priority to avoid blocking other jobs in our queue.</p>

<p>The other types of notifications were all implemented similarly (via an observer on the model, and a specific <code>Notification</code> class inheriting from the base class). We also added some other logic into the base <code>Notification</code> class such as some uniqueness constraints, as well as an ability to mark notifications as 'sent' or 'invalid'. However, we ran into serious performance/scaling issues fairly quickly, and had to re-implement this scheme.</p>

<a name="Performance.Bottlenecks"></a>
<h3>Performance Bottlenecks</h3>

<p>All of these records were being written to one collection in <a href="https://www.mongodb.org/">MongoDB</a>, and the size of this collection grew quite rapidly. It's size almost immediately dwarfed the size of any of our other collections, and the number of records quickly reached into the tens of millions. This led to problems: writing new notifications started to crawl to a standstill. We had several indices on this collection to aid in querying, and these made the insertion of new notifications very non-performant, and also started to affect overall database performance. Querying against this collection degraded quickly and started to similarly affect database performance. Archiving old records also proved next to impossible. We couldn't simply drop the entire collection, but had to prune records. This similarly was totally non-performant and was adversely affecting database and site performance. We needed to come up with a new implementation for <code>Notification</code>, and addressing these issues was essential.</p>

<a name="Resolving.Performance.Bottlenecks"></a>
<h3>Resolving Performance Bottlenecks</h3>

<p>So, we decided on a scheme where each day would result in a new <code>Notifications</code> collection (name keyed on the date), named <code>notifications_20140101</code>, <code>notifications_20140102</code>, etc. Each of these collections would have an <code>_id</code> field that corresponds to a user_id, and an <code>events</code> array (or 'stack' if you will) that records the id's of notified objects, as well as the type of notification. An example of a record in that collection is:</p>

<pre><code class="json">{"_id"=&gt;"5106b619f56337db300001f8",
 "events"=&gt;[{"t"=&gt;"NearbyShow", "o"=&gt;"533998b1c9dc24c371000041"},
            {"t"=&gt;"NearbyShow", "o"=&gt;"5345774cc9dc246d580003d0"},
            {"t"=&gt;"NearbyShow", "o"=&gt;"5335af4fa09a67145300028c"},
            {"t"=&gt;"NearbyShow", "o"=&gt;"533f1174a09a67298900007b"},
            {"t"=&gt;"ArtworkPublished", "o"=&gt;"5334647b139b2165160000d8"}]
}
</code></pre>

<p>So, here we see all of my notifications for April 22, 2014. On that day, I was notified about 4 shows opening near my location, and one artwork added by an artist I follow. Incidentally, that artwork was a piece by <a href="https://artsy.net/artist/rob-wynne">Rob Wynne</a> entitled <a href="https://artsy.net/artwork/rob-wynne-youre-dreaming">You're Dreaming</a>. The show notifications were for NYC-area shows opening at <a href="https://artsy.net/klein-sun-gallery">Klein Sun Gallery</a>, <a href="https://artsy.net/garis-and-hahn">Garis &amp; Hahn</a>, <a href="https://artsy.net/miyako-yoshinaga-gallery">Miyako Yoshinaga Gallery</a> and <a href="https://artsy.net/dodgegallery">DODGEgallery</a>.</p>

<p>A couple of nice things about this implementation is it limits the size of a collection: any one day's collection will scale directly with the number of users, which seems reasonable. Our earlier implementation scaled with the product of the number of users and amount of content on Artsy, which is clearly problematic. Also, archiving old notifications is as simple as dropping a particular day's collection, which is very performant. However, querying and assembling these notifications is a bit trickier than in the naive implementation, as well as marking which events have already been sent to a user, so as to avoid duplicating any content in between mailings.</p>

<p>Let's see how we rewrite the notification generation in this scheme:</p>

<pre><code class="ruby">module NotificationService

  def self.notify_many!(user_ids, object_id, type)
    events = events_from(object_id, type)
    user_ids.each do |user_id|
      notify_with_events(user_id, events)
    end
  end

  private

  def self.notify_with_events(user_id, events)
    collection.find(_id: user_id).upsert('$push' =&gt; { events: { '$each' =&gt; events } })
  end

  def self.events_from(object_ids, type)
    Array(object_ids).map do |object_id|
      {
        't' =&gt; type,
        'o' =&gt; object_id
      }
    end
  end

  # collection storing notifications for the given day
  def self.collection(date = Date.today)
    Mongoid.default_session.with(safe: false)[collection_name(date)]
  end

  def self.collection_name(date)
    "notifications_#{date.to_s(:number)}"
  end

end
</code></pre>

<p>Here's how the <code>after_save</code> callback looks now:</p>

<pre><code class="ruby">def notify_published
  NotificationService.notify_many!(user_ids, self, 'ArtworkPublished') if self.published_changed? &amp;&amp; self.published
end
</code></pre>

<p>Let's take a look at what's going on here. When an artwork is published, we call <code>notify_many!</code> in the <code>NotificationService</code> module. That will determine the correct collection (keyed by the date) using the <code>collection</code> and <code>collection_name</code> helpers. We build our events stack with the <code>events_from</code> helper, and then do an <code>upsert</code> with a <code>$push</code> to either insert or update that user's events for that day. Due to the fast performance of this scheme, we also no longer have to batch notification creation. As a sample benchmark, writing this type of notification to our <a href="https://artsy.net/artist/andy-warhol">Warhol</a> followers takes under 15 seconds.</p>

<p>Ok, so we seem to have solved some of our issues: namely writing and archiving notifications is performant, and we understand the behavior of these collections as the number of users and content on the site grows. Now let's look at how we can query against this scheme in an efficient manner, and also how we can mark events as 'seen' to avoid emailing out duplicates.</p>

<a name="Marking.Notifications.as.Flushed.and.Retrieving.Notifications"></a>
<h3>Marking Notifications as Flushed and Retrieving Notifications</h3>

<p>We decided to push a <code>flushed</code> event onto the user's stack after we send out notifications, and analogously, when we are querying a user's notifications, we want to throw away notifications that occur before a <code>flushed</code> event. Here's that method, in our <code>NotificationService</code> module:</p>

<pre><code class="ruby"># Mark all events until this point "seen." Pushes a {flushed: &lt;id&gt;}
# hash on to events array.
def self.flush!(user_id, since = Date.today - 7.days)
  flushed = { 'flushed' =&gt; Moped::BSON::ObjectId.new }
  collections_since(since).each do |coll|
    coll.find(_id: user_id).update('$push' =&gt; { events: flushed })
  end
  flushed  # return "id" of flushed marker, in case useful later
end

private

def self.collections_since(date)
  (date..Date.today).map { |d| collection(d) }
end
</code></pre>

<p>Pretty simple. We push the appropriate event onto every collection that was under consideration via the <code>collections_since</code> helper. When we send out a personalized email we accumulate the last 7 day's worth of activity for you, and so after we generate/send a mail for a user, we can simply say <code>NotificationService.flush!(user)</code>. Here's how that day's notifications for me looks after the <code>flushed</code> event:</p>

<pre><code class="json">  {"_id"=&gt;"5106b619f56337db300001f8",
   "events"=&gt;[{"t"=&gt;"NearbyShow", "o"=&gt;"5338504e139b21f2a9000362"},
              {"t"=&gt;"FollowedArtistShow", "o"=&gt;"533ddba3a09a6764f60006b6"}, {"t"=&gt;"NearbyShow", "o"=&gt;"533ddba3a09a6764f60006b6"},
              {"flushed"=&gt;"5352b346b504f5f3690002fe"}]
  }
</code></pre>

<p>For the last piece of the puzzle, let's look at how we query against this scheme and compile together all notifications that are applicable for a given user:</p>

<pre><code class="ruby">module NotificationService
  NOTIFICATION_TYPES = {
    'FollowedArtistShow' =&gt; PartnerShow,
    'FollowedPartnerShow' =&gt; PartnerShow,
    'NearbyShow' =&gt; PartnerShow,
    'ArtworkPublished' =&gt; Artwork,
    'ArtworkSuggested' =&gt; Artwork
  }

  class Notification &lt; Struct.new(:type, :object_id)
    def object
      @object ||= NOTIFICATION_TYPES[type].find(object_id)
    end

    def applicable?
      object.try(:notifiable?) || false
    end
  end

# Return applicable notifications for user since given date.
def self.get(user_id, since = Date.today - 7.days)
  collections_since(since)
    .map { |coll| coll.find(_id: user_id).one }.compact
    .flat_map { |doc| doc['events'].slice_before { |ev| ev['flushed'] }.to_a.last }
    .reject { |ev| ev['flushed'] }.uniq
    .map { |ev| Notification.new(ev['t'], ev['o']) }.select(&amp;:applicable?)
end
</code></pre>

<p>We introduce a lite-weight <code>Notification</code> class that will load the object, as well as perform an additional check. We use the previously introduced <code>collections_since</code> helper to retrieve all the notification collections under consideration. We query each and build up an array of all events from a user's stack. We remove events that occurred prior to a <code>flushed</code> event in a given collection and the <code>flushed</code> events themselves. Then we actually load all the objects and return the ones that are still <code>applicable?</code>. That final <code>applicable?</code> check is to allow us to filter out content at run-time that is no longer valid. For example, if an artwork is published and the correct event is written out to users, but before the user can be notified the artwork is unpublished, this can serve as a run-time check to not include that work. <code>def notifiable?</code> can thus be implemented in the <code>Artwork</code> model like so:</p>

<pre><code class="ruby">def notifiable?
  published?
end
</code></pre>

<p>And...that's basically it! Throughout the week as partners are uploading their shows/fair booths/artworks, these records are being opportunistically written to that day's notification collection, in a performant and scalable fashion. Then when we want to send you a personalized email, we pull all your appropriate notifications via the <code>get</code> routine in our <code>NotificationService</code>, and primarily using the technique described in <a href="http://artsy.github.io/blog/2014/03/18/presenters-and-memoization-moving-logic-out-of-templates/">Presenters and Memoization</a> we make sure we cache/memoize all such data. Using the tips in <a href="http://artsy.github.io/blog/2014/03/17/ruby-helper-to-group-artworks-into-a-pinterest-style-layout-for-email/">Pinterest-style Layouts</a> and <a href="http://artsy.github.io/blog/2014/03/17/some-tips-for-email-layout-and-responsiveness/">Email Layouts and Responsiveness</a> we can render this content and support various devices/email clients. We parallelize and batch the generation/sending of our e-mails as well. This whole system, from notification generation to actually emailing users, is running successfully and smoothly in production.</p>

<a name="Next.Steps"></a>
<h3>Next Steps</h3>

<p>I think this type of infrastructure can easily be adapted to serve as a feed on a front-end or other client app. An API to serve up these notifications (AKA feed items) can be built and different feed items can then be rendered or aggregated at load-time. Simple client-side polling can even be set up to alert a user if something has happened that interests them <em>while</em> they're browsing! I think push notifications and other messaging can be handled by this system as well.</p>

<p>I'd love to hear any feedback and thoughts, and hopefully you've found this post informative and interesting. Please leave any feedback in the comments and <a href="https://github.com/artsy">follow us on Github</a>!</p>
]]></content>
  </entry>
  
</feed>
