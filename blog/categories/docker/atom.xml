<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docker | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2018-12-16T10:16:31+00:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Stamping the commit SHA into the ENV vars of a running Docker-based app]]></title>
    <link href="http://artsy.github.io/blog/2018/09/10/Dockerhub-Stamping-Commits/"/>
    <updated>2018-09-10T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2018/09/10/Dockerhub-Stamping-Commits</id>
    <content type="html"><![CDATA[<p>For what feels like the last 3-6 months, I've been trying to figure out how to know what the commit is for the
Docker runtime in Peril. Roughly: every master commit on Peril triggers a Docker image on Docker Hub for the
environment in which JavaScript is running. There's a lag between creating the commit, having the image ready on
Docker Hub, and Peril using the new image. There's also space for these automated systems to go wrong, so I'd like
to be able to be certain in logging.</p>

<p>I've thrown a lot of commits and time every few weeks at this, so now that I've figured it out, I'll give you an
idea of what I needed to do to make it work in a micro-post.</p>

<!-- more -->


<p><strong>Step 1:</strong> You need a custom build step, to do this, you need to create a file <code>hooks/build</code> in your repo:</p>

<pre><code class="sh">#!/usr/bin/env sh

# This is so we can get the commit into the build log of a Dangerfile runner
# These come from https://docs.docker.com/docker-cloud/builds/advanced/

# For debugging all env vars
# printenv

#  Convert the location "/Dockerfile" to "Dockerfile"
FILE=$(echo -n $BUILD_PATH | tail -c +2)

if [ -z "${DOCKER_TAG}" ]; then
  docker build --build-arg=COMMIT=$(git rev-parse --short HEAD) --build-arg=BRANCH=$SOURCE_BRANCH -t $IMAGE_NAME -f $FILE .
else
  docker build --build-arg=COMMIT=$(git rev-parse --short HEAD) --build-arg=BRANCH=$DOCKER_TAG -t $IMAGE_NAME -f $FILE .
fi
</code></pre>

<p>There's a list of examples in <a href="https://github.com/thibaultdelor/testAutobuildHooks">this repo</a> - though the build
one is too simple for our needs here. If you need something that's not there, then remove the comment marker before
<code>printenv</code> to the script to see
<a href="https://github.com/danger/peril/commit/61f447d13476fee9fa0686225ff3ca76d416088f">what env vars</a> are available
(<a href="https://hub.docker.com/r/dangersystems/peril/builds/benoxzftncgdsmwugr9bpjn/">here's an example build</a>).</p>

<p><strong>Step 2:</strong> Edit your <code>Dockerfile</code> to take the additional arguments <code>COMMIT</code> and <code>BRANCH</code> from <code>ARG</code>.</p>

<pre><code class="diff">MAINTAINER Orta Therox
+ ARG BRANCH="master"
+ ARG COMMIT=""
+ LABEL branch=${BRANCH}
+ LABEL commit=${COMMIT}

ADD . /app
WORKDIR /app

+ # Now set it as an env var
+ ENV COMMIT_SHA=${COMMIT}
+ ENV COMMIT_BRANCH=${BRANCH}
</code></pre>

<p>Err, that should be everything. I mean, I did call it a micro-post. Trying to implement this has broken the Peril
runner a bunch of times on staging, so I'm mainly just helping out other docker newbies.</p>

<p>Some links that helped me get there:</p>

<ul>
<li><a href="https://github.com/docker/hub-feedback/issues/600">Add git commit hash to ENV</a></li>
<li><a href="https://github.com/docker/hub-feedback/issues/508#issuecomment-243968310">Feature request: Build args on docker hub</a></li>
<li><a href="https://github.com/elasticdog/tiddlywiki-docker/commit/993c7e9e8d5207d110270458f0f18839656ca126">Inject Git source commit metadata into the image</a></li>
<li><a href="https://docs.docker.com/docker-hub/github/">Configure automated builds from GitHub</a></li>
</ul>


<p>Remember folks, Ash says you should <a href="https://ashfurrow.com/blog/contemporaneous-blogging/">write as you learn</a>, so
write up those small wins.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Docker and Dusty for Development]]></title>
    <link href="http://artsy.github.io/blog/2015/12/09/docker-for-development/"/>
    <updated>2015-12-09T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/12/09/docker-for-development</id>
    <content type="html"><![CDATA[<p>When I first proposed using Docker for development, and began doing my work that way, there were some doubts.</p>

<ul>
<li>Doesn't it seem like a lot of trouble to set up Docker to get my work done?</li>
<li>Isn't it easier to use <a href="http://brew.sh/">Homebrew</a> to install the services and database servers I need?</li>
</ul>


<!-- more -->


<p>At Artsy, our main API aka Gravity uses MongoDB, Solr, Elasticsearch, and memcached. In development, we use <a href="http://mailcatcher.me/">Mailcatcher</a> so we can view emails. When a new software engineer starts, that person studies a big Getting Started document, and spends most of a day to get everything installed and configured. Not only do they need to get the software installed, figuring out all of the environment variables that need to be set up can take some time too. While we have good documentation, it is still a tedious and repetitive process that takes up the time of our new employee, and more experienced developers who need to answer questions.</p>

<p>Now that Gravity has been dockerized, getting set up consists of a one-time install of <a href="https://www.docker.com/docker-toolbox">Docker Toolbox</a> followed by running</p>

<pre><code class="bash">docker-compose build &amp;&amp; docker-compose up
</code></pre>

<p>in the root directory of the checked-out repo. Here is a simplified version of our <a href="https://docs.docker.com/compose/">docker-compose</a> setup. Because we run a web server and a delayed_job process, <code>docker-compose.yml</code> uses a <code>common.yml</code> file for shared setup:</p>

<pre><code class="yaml">gravity:
  build: .
  environment:
    MEMCACHE_SERVERS: memcached
    SOLR_URL: http://solr4:8983/solr/gravity
    MONGO_HOST: mongodb
    ELASTICSEARCH_URL: elasticsearch:9200
    SMTP_PORT: 1025
    SMTP_ADDRESS: mailcatcher
  env_file: .env
</code></pre>

<p>The <code>.env</code> file is used for secrets such as Amazon Web Services credentials we don't want to put into the git repository.</p>

<p>Our <code>docker-compose.yml</code> looks like this:</p>

<pre><code class="yaml">mongodb:
  image: mongo:2.4
  command: bash -c "rm -f /data/db/mongod.lock; mongod --smallfiles --quiet --logpath=/dev/null"
  ports: 
  - "27017:27017"

solr4:
  image: artsy/solr4

memcached:
  image: memcached

elasticsearch:
  image: artsy/elasticsearch
  ports:
  - "9200:9200"
  - "9300:9300"

web:
  extends:
    file: common.yml
    service: gravity
  command: script/rails s -b 0.0.0.0 -p 80
  ports:
  - "80:80"
  volumes:
  - .:/app
  links:
  - elasticsearch
  - mongodb
  - memcached
  - solr4
  - mailcatcher

dj:
  extends:
    file: common.yml
    service: gravity
  command: bundle exec rake jobs:work
  volumes:
  - .:/app
  links:
  - elasticsearch
  - mongodb
  - memcached
  - solr4

mailcatcher:
  image: zolweb/docker-mailcatcher
  ports:
  - "1080:1080"
</code></pre>

<p>The command for the MongoDB section removes a lock file that can remain in place sometimes when the container is killed. Do not use that in production! We mount the local directory into the container with a <code>volumes:</code> command, so that local changes are reloaded in the running containers.</p>

<p>Recently, <a href="https://github.com/ashkan18">Ashkan Nasseri</a> began to move our delayed jobs from <a href="https://github.com/collectiveidea/delayed_job_mongoid">delayed_job_mongoid</a> to <a href="http://sidekiq.org/">sidekiq</a>, which brings in Redis and another process that needs to run during development. Since we are using Docker, all we have to do is add a couple of new sections to our <code>docker-compose.yml</code> file:</p>

<pre><code class="yaml">redis:
  image: redis
  ports:
  - "6379:6379"

sidekiq:
  extends: 
    file: common.yml
    service: gravity
  command: bundle exec sidekiq
  volumes:
  - .:/app
  links:
  - elasticsearch
  - mongodb
  - memcached
  - solr4
  - redis
</code></pre>

<p>and add this line to <code>common.yml</code>:</p>

<pre><code class="yaml">REDIS_URL: redis://redis
</code></pre>

<p>The next time someone runs <code>docker-compose up</code>, this will cause a one-time download of a redis image, and then it brings up the additional sidekiq service.</p>

<p>For development which involves multiple applications in separate git repositories, we use <a href="http://dusty.gc.com/">Dusty</a>, which was created by <a href="https://gc.com/">GameChanger</a>. Some of the advantages of using Dusty include the use of NFS (which performs much better than shared volumes in VirtualBox), and a built-in nginx proxy along with modifications to your <code>/etc/hosts</code> file so that you can more easily connect to your applications.</p>

<p>With Dusty, you set up services, apps, and bundles of apps with YAML files. Here is a repo with <a href="https://github.com/gamechanger/dusty-example-specs">sample Dusty specs</a>.</p>

<p>Our MongoDB service is defined as:</p>

<pre><code class="yaml"># services/mongo2.yml
image: mongo:2.4
volumes:
- /persist/persistentMongo:/data/db
entrypoint: ["sh", "-c", "rm -f /data/db/mongod.lock; mongod --smallfiles --quiet --logpath=/dev/null"]
ports:
- "27017:27017"
</code></pre>

<p>It's not necessary to expose the ports, but in case we want to connect directly to the MongoDB instance with the <code>mongo</code> command without shelling into a container, we need it to be available on our Docker VM's IP address.</p>

<p>Our Gravity app's Dusty YAML file is:</p>

<pre><code class="yaml"># apps/gravity.yml
repo: github.com/artsy/gravity
mount: /app
build: .

depends:
  services:
  - mongo2
  - memcached
  - solr4
  - es15
  - mailcatcher
  - redis
  apps:
  - radiation

host_forwarding:
- host_name: gravity
  host_port: 80
  container_port: 80

compose:
  environment:
    RADIATION_URL: http://radiation
    MONGO_HOST: mongo2
    MEMCACHE_SERVERS: memcached
    SOLR_URL: http://solr4:8983/solr/gravity
    ELASTICSEARCH_URL: es15:9200
    SMTP_ADDRESS: mailcatcher
    SMTP_PORT: 1025
    REDIS_URL: redis://redis

commands:
  once:
  - bundle install -j 10
  - bundle exec rake db:client_applications:create_all
  - bundle exec rake db:admin:create
  always:
  - rails s -b 0.0.0.0 -p 80
</code></pre>

<p>The <code>depends:</code> configuration is similar to the <a href="https://docs.docker.com/compose/compose-file/#links">links</a> functionality of docker-compose. It makes sure that those applications (as defined in <code>apps/*.yml</code>) are running, and sets up <code>/etc/hosts</code> in the containers to allow your applications to refer to other services using their hostnames.</p>

<p>For now, Dusty doesn't have a way of sharing common setup like <code>common.yml</code> above, so there are similar configurations for our Sidekiq and Delayed Job workers.</p>

<p>Dusty uses bundles for clusters of applications that need to work together. An example bundle, for a CMS application that needs many APIs, is:</p>

<pre><code class="yaml"># apps/volt.yml
description: Volt
apps:
  - tangentApi
  - radiation
  - superposition
  - gravity
  - volt
</code></pre>

<p>We bring up that cluster of applications with</p>

<pre><code class="bash">dusty bundles activate volt
dusty up
</code></pre>

<p>As we have added new services over time, using Docker and Dusty to bring clusters of apps together has made it much easier for developers to work on projects without having to spend time on installation and configuration. Having Docker configuration in the repo also serves as good (and up-to-date) documentation of how a given application is configured and its dependencies. It is also much less resource-intensive compared to using virtual machines configured with Vagrant or another provisioning tool. All of our Docker applications and services can run in a single VM. If you are developing on Linux, you don't even need a VM!</p>

<p>We are also starting to use Docker to run integrated testing across multiple applications using Selenium. That will be covered in a future blog post.</p>
]]></content>
  </entry>
  
</feed>
