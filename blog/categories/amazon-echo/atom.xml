<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: amazon echo | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/amazon-echo/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2018-12-16T10:16:31+00:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Bringing Artsy to Amazon Echo "Alexa"]]></title>
    <link href="http://artsy.github.io/blog/2016/11/30/bringing-artsy-to-amazon-echo-alexa/"/>
    <updated>2016-11-30T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2016/11/30/bringing-artsy-to-amazon-echo-alexa</id>
    <content type="html"><![CDATA[<p>tl;dr You can try Artsy on your Amazon Echo now, say "Alexa, enable Artsy" or see <a href="http://alexa.artsy.net">alexa.artsy.net</a> for more info.</p>

<p>With its powerful automatic speech recognizer, accurate natural language understanding and outstanding text-to-speech capabilities, the Amazon Echo, nicknamed "Alexa", always impressed me. While not the first in its category and introduced in late 2014, Alexa was the first consumer device in my home to truly enable the conversation between human and machine. It was stationary, always listening for a wake word, and clearly outperformed all previous attempts when it came to the ability to receive commands from the other side of the apartment.</p>

<p>Alexa knows about the weather, but it doesn't know much about art.</p>

<p>In this post I'll dig a little inside the Alexa software platform and go over the technical details of bringing Artsy to the Echo, starting with a very simple "Ask Artsy about Norman Rockwell."</p>

<p><a href='https://www.youtube.com/watch?v=zi3OubNiV9U' target='_blank'></p>

<iframe width="280" height="280" src="https://www.youtube.com/embed/zi3OubNiV9U" frameborder="0" allowfullscreen></iframe>


<p></a></p>

<p>Find the <a href="http://alexa.amazon.com/spa/index.html#skills/dp/B01MYLJO1N">Artsy skill in the Alexa app</a> and <a href="https://github.com/artsy/elderfield">the complete Skill code on Github</a>. And if you just want to learn to write a skill quickly, watch <a href="https://www.youtube.com/watch?v=pzM4jv7k7Rg">@dblock live-code a skill in this video</a>.</p>

<!-- more -->


<a name="How.does.Alexa.work."></a>
<h3>How does Alexa work?</h3>

<p><em>Alexa</em> is the software platform used by Alexa-enabled devices such as the <em>Amazon Echo</em> or the <em>Echo Dot</em>. I used a $49.- Dot with headphones for most testing, and I have a bigger Echo at home (the main difference is the quality of the speaker). Because even a smaller Dot was sufficiently annoying to my coworkers, I wish it were battery powered, so I wouldn't need to unplug it to go run tests in a conference room.</p>

<p>Alexa waits for a <em>wake word</em>, either "Alexa" or "Amazon". It's always on and has a built-in speech recognizer for the wake word only. After it hears the wake word, a blue light ring turns on and the device starts transmitting the user's voice, called an <em>utterance</em> to the Alexa platform in the cloud. The light ring is the indicator of it "listening". The cloud service will translate speech to text and run it through a natural language system to identify an <em>intent</em>, such as "ask Artsy about". The intent is sent to a <em>skill</em>, which is a piece of software living in the cloud that can generate a <em>directive</em> to "speak" along with markup in <a href="https://www.w3.org/TR/speech-synthesis">SSML</a> format, transformed into voice and finally sent in wave format back to the device to be played back to the user.</p>

<a name="Developer.Prerequisites"></a>
<h3>Developer Prerequisites</h3>

<p>You need an <a href="https://developer.amazon.com">Amazon Apps and Services Developer account</a> and access to <a href="https://console.aws.amazon.com/lambda">AWS Lambda</a>. I've implemented the Artsy skill in node.js using <a href="https://github.com/matt-kruse/alexa-app">alexa-app</a>. You can find the <a href="https://github.com/artsy/elderfield">complete Skill code on Github</a>.</p>

<a name="Designing.an.Intent"></a>
<h3>Designing an Intent</h3>

<p>To get Alexa to listen, you must design an intent. Each intent is identified by a <em>name</em> and a set of <em>slots</em>. This is an order of magnitude more limited than what the Alexa system is capable of doing out of the box. It means that the platform <em>will not</em> transcribe any text for your application, chatbot-style. The intents have to be simple and clear and use English language words or predefined vocabularies. Furthermore, Alexa has a lot of trouble with names as it constantly attempts to recognize them as dictionary words and no support for "wildcards".</p>

<p>I started implementing a fairly complicated "when is an artist born" intent, which turned to be too ambitious for Alexa to understand and ended up with a much simpler "ask artsy about an artist". I called this "AboutIntent", which takes an artist name as input (or slot).</p>

<pre><code class="json">{
   "intents": [
      {
         "intent": "AboutIntent",
         "slots": [
            {
               "name": "VALUE",
               "type": "NAME"
            }
         ]
      }
   ]
}
</code></pre>

<p>The only possible sample utterance of this intent is "about {VALUE}", the "ask Artsy" portion is implied.</p>

<p>Since Alexa cannot understand artist names out of the box, I had to teach it with a custom, user-defined slot type added to the "Skill Interaction Model" with about a thousand most popular artist names on Artsy.</p>

<p><img src="/images/2016-11-30-bringing-artsy-to-amazon-echo-alexa/alexa-interaction-model.png" alt="Alexa interaction model" /></p>

<a name="Implementing.a.Skill"></a>
<h3>Implementing a Skill</h3>

<p>Intents are the API of a skill, otherwise known as an Alexa app. Using <a href="https://github.com/matt-kruse/alexa-app">alexa-app</a> makes implementing intents easy.</p>

<pre><code class="js">var alexa = require('alexa-app');
var app = new alexa.app('artsy');

app.launch(function(req, res) {
    res
        // welcome message
        .say("Welcome to Artsy! Ask me about an artist.")
        // don't close the session, wait for user input (an artist name)
        // and provide a re-prompt in case the user says something meaningless
        .shouldEndSession(false, "Ask me about an artist. Say help if you need help or exit any time to exit.")
        // speak the response
        .send();
});

app.intent('AboutIntent', {
        "slots": {
            "VALUE": "NAME"
        },
        "utterances": [
            "about {-|VALUE}"
        ]
    },
    function(req, res) {
      // intent implementation goes here
    }
});
</code></pre>

<p>The skill expects a slot value as defined in the intent above.</p>

<pre><code class="js">var value = req.slot('VALUE');

if (!value) {
  return res
    .say("Sorry, I didn't get that artist name.")
    // don't close the session, wait for user input again (an artist name)
    .shouldEndSession(false, "Ask me about an artist. Say help if you need help or exit any time to exit.");
} else {
  // asynchronously lookup the artist in the Artsy API, read their bio
  // tell alexa-app that we're performing an asynchronous operation by returning false
  return false;
}
</code></pre>

<p>We use the <a href="https://developers.artsy.net">Artsy API</a> to implement the actual skill. There's not much more to it.</p>

<a name="Skill.Development.Mode"></a>
<h3>Skill Development Mode</h3>

<p>I found it convenient to host the skill inside an <a href="https://github.com/matt-kruse/alexa-app-server">alexa-app-server</a> in development. It automatically loads skills from subdirectories with the following directory structure.</p>

<pre><code>+--- server.js                  // the alexa-app-server host for development
+--- package.json               // dependencies of the host
+--- project.json               // lambda settings for deployment with apex
+----functions                  // all skills
     +--artsy                   // the artsy skill
        +--function.json        // describes the skill lambda function
        +--package.json         // dependencies of the skill
        +--index.js             // skill intent implementation
        +--schema.json          // exported skill intent schema
        +--utterances.txt       // exported skill uterrances
        +--node_modules         // modules from npm install
+--- node_modules               // modules from npm install
</code></pre>

<p>Note that the server exports the Express.js server for testing.</p>

<pre><code class="js">var AlexaAppServer = require('alexa-app-server');

AlexaAppServer.start({
    port: 8080,
    app_dir: "functions",
    post: function(server) {
        module.exports = server.express;
    }
});
</code></pre>

<a name="Skill.Modes"></a>
<h3>Skill Modes</h3>

<p>The skill is mounted standalone in AWS lambda, under alexa-app-sever in development and otherwise will export schema and utterances. It decides what to do based on <code>process.env['ENV']</code>, loaded from a ".env" file. The committed version of this file sets the value to "development".</p>

<pre><code>{
    "ENV": "development"
}
</code></pre>

<p>The server loads <code>.env</code> from <code>server.js</code>.</p>

<pre><code class="js">var config = require('./.env.json')

for (var key in config) {
    process.env[key] = config[key]
}
</code></pre>

<p>Finally, the skill can do different things in different environments.</p>

<pre><code class="js">if (process.env['ENV'] == 'lambda') {
    exports.handle = app.lambda(); // AWS lambda
} else if (process.env['ENV'] == 'development') {
    module.exports = app; // development mode
} else {
    var fs = require('fs');
    // export schema and utterances
    // copy-paste the contents into the Interaction Model
    fs.writeFileSync('schema.json', app.schema());
    fs.writeFileSync('utterances.txt', app.utterances());
}
</code></pre>

<a name="Development.and.Test"></a>
<h3>Development and Test</h3>

<p>Running the server locally with <code>node server.js</code> will create a simulator console on http://localhost:8080/alexa/artsy.</p>

<p><img src="/images/2016-11-30-bringing-artsy-to-amazon-echo-alexa/console.png" alt="lambda configuration" /></p>

<p>We will use the test console to produce test JSON. The about intent translates into the following.</p>

<pre><code class="json">{
    "session": {
        "sessionId": "SessionId...",
        "application": {
            "applicationId": "amzn1.ask.skill..."
        },
        "attributes": {},
        "user": {
            "userId": "amzn1.ask.account..."
        },
        "new": true
    },
    "request": {
        "type": "IntentRequest",
        "requestId": "EdwRequestId...",
        "locale": "en-US",
        "timestamp": "2016-11-12T22:03:54Z",
        "intent": {
            "name": "AboutIntent",
            "slots": {
                "VALUE": {
                    "name": "VALUE",
                    "value": "Norman Rockwell"
                }
            }
        }
    },
    "version": "1.0"
}
</code></pre>

<p>A mocha test uses the Alexa app server to make an HTTP request using the above intent data and expects well defined SSML output.</p>

<pre><code class="js">chai = require('chai');
expect = chai.expect;
chai.use(require('chai-string'));
chai.use(require('chai-http'));

var server = require('../server');

describe('artsy alexa', function() {
    it('tells me about Norman Rockwell', function(done) {
        var aboutIntentRequest = require('./AboutIntentRequest.json');
        chai.request(server)
            .post('/alexa/artsy')
            .send(aboutIntentRequest)
            .end(function(err, res) {
                expect(res.status).to.equal(200);
                var data = JSON.parse(res.text);
                expect(data.response.outputSpeech.type).to.equal('SSML')
                var ssml = data.response.outputSpeech.ssml;
              expect(ssml).to.startWith('&lt;speak&gt;American artist Norman Rockwell ');
              done();
            });
    });
});
</code></pre>

<a name="Lambda.Deployment"></a>
<h3>Lambda Deployment</h3>

<p>The production version of the Alexa skill is a Lambda function.</p>

<p>I created an "alexa-artsy" function with a new IAM role, "alexa-artsy" in AWS Lambda and copy-pasted the role URN into "project.json". This is a file used by <a href="https://github.com/apex/apex">Apex</a>, a Lambda deployment tool (<code>curl https://raw.githubusercontent.com/apex/apex/master/install.sh | sh</code>) along with awscli (<code>brew install awscli</code>). I had to configure access to AWS the first time, too (<code>aws configure</code>).</p>

<p><img src="/images/2016-11-30-bringing-artsy-to-amazon-echo-alexa/lambda-configuration.png" alt="lambda configuration" /></p>

<p>In order to connect the lambda function with an Alexa skill, I added an "Alexa Skills Kit" trigger. Without this you get an obscure "Please make sure that Alexa Skills Kit is selected for the event source type of arn:..." error.</p>

<p><img src="/images/2016-11-30-bringing-artsy-to-amazon-echo-alexa/lambda-triggers.png" alt="lambda triggers" /></p>

<p>I configured the "Service Endpoint" in the Alexa Skill configuration to point to my Lambda function.</p>

<p><img src="/images/2016-11-30-bringing-artsy-to-amazon-echo-alexa/alexa-skill-configuration.png" alt="Alexa skill configuration" /></p>

<p>I deployed the lambda function with <code>apex deploy</code> and test the skill with <code>apex invoke</code> or from the Alexa test UI.</p>

<p>Each time we do a <code>apex deploy</code> it will create a new version of our function and will update <code>$LATEST</code> <a href="http://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html">Lambda alias</a>. One way to handle versioning is to create a <code>$PROD</code> Lambda alias and point to <code>$PROD</code> when setting function in "Service Endpoint" step and each time we want to upate prod we need to update <code>$PROD</code> alias to point to latest version.</p>

<p>Also note that after each deploy with apex we need to set environment variables for the newly created version.</p>

<p>Logs didn't appear in AWS Cloudwatch with the execution policy created by default, I had to give the IAM "alexa-artsy" role more access to "arn:aws:logs:<em>:</em>:*" via an additional inline policy.</p>

<pre><code class="json">{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:*:*:*"
    }
  ]
}
</code></pre>

<a name="Testing.with.Echoism"></a>
<h3>Testing with Echoism</h3>

<p>You can use <a href="https://echosim.io">echosim.io</a> to test your skill. It's essentially identical to an Echo.</p>

<a name="Testing.with.Echo"></a>
<h3>Testing with Echo</h3>

<p>I got an Echo Dot. Test skills appear automatically in the Alexa configuration attached to my account!</p>

<p><img src="/images/2016-11-30-bringing-artsy-to-amazon-echo-alexa/alexa-skills.png" alt="alexa skills" /></p>

<p>I just try to <a href="https://www.youtube.com/watch?v=FYVOAU35Sio">talk to it</a>.</p>

<a name="Certification.Process"></a>
<h3>Certification Process</h3>

<p><img src="/images/2016-11-30-bringing-artsy-to-amazon-echo-alexa/certified-email.png" alt="certified email" /></p>

<p>This took several weeks of back-and-forth. I should have gone through the <a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-voice-interface-and-user-experience-testing">Alexa Skills Kit Voice Interface and User Experience Testing for Custom Skills</a> more thoroughly, but even after reading it a few times it wasn't obvious what the reviewers wanted.</p>

<p><img src="/images/2016-11-30-bringing-artsy-to-amazon-echo-alexa/certified-dashboard.png" alt="certified dashboard" /></p>

<p>Keep in mind that you need to go through certification process only if you are changing descriptions and details of the skill. If you updated the lambda function without modifying <code>utterances</code> or <code>schema</code> you don't have to go through certification process.</p>

<p>Here's a list of my mistakes.</p>

<a name="Sample.Phrases"></a>
<h4>Sample Phrases</h4>

<p>The <em>Example Phrases</em> in the <em>Publishing Information</em> section of the certification UI must be present in your sample utterances. My three examples are <em>"Alexa, ask Artsy about Artsy"</em>, <em>"Alexa, ask Artsy about Andy Warhol"</em> and <em>"Alexa, ask Artsy about Norman Rockwell"</em>.</p>

<p>The sample utterances are in the <em>Interaction Model</em> section and must not include the wake word or any relevant launch phrasing. In the Artsy app I only have a single utterance with <em>"AboutIntent about {VALUE}"</em>, but lots of entries for <em>VALUE</em> in the custom slot type. I forgot to include <em>artsy</em> and <em>Norman Rockwell</em> in those.</p>

<p>The skill got rejected mutliple times with <em>about artsy</em> not working, so I eventually gave up on supporting that. Alexa would recognize it as <em>artsee</em>, <em>artzi</em> and many other variations. I suspect testers pronounce it <em>artsay</em> or <em>artsai</em> and never get the correct response.</p>

<a name="The.Opening.Line"></a>
<h4>The Opening Line</h4>

<p>The welcome prompt is required, and must describe what users can ask of the skill and the session must remain open for a user response. This means that in order to get your app certified you must build some kind of basic conversational ability. My first cut simply said <em>"Welcome to Artsy"</em>, which wasn't good enough. I added a prompt, <em>"Ask me about an artist."</em> and included <code>shouldEndSession(false)</code> in the code.</p>

<a name="Stop.and.Cancel.Intents"></a>
<h4>Stop and Cancel Intents</h4>

<p>The skill must exit appropriately when users say <em>"stop"</em> or <em>"cancel"</em>. This means you must build at the very least an <em>AMAZON.StopIntent</em>, <em>AMAZON.CancelIntent</em>. In my first iteration a user would say <em>"Alexa open Artsy"</em>, then <em>"stop"</em> and would get <em>"Sorry, I didn't get that artist name."</em>.</p>

<a name="Help.Intent"></a>
<h4>Help Intent</h4>

<p>The skill must also implement <em>AMAZON.HelpIntent</em> as a question. In my first iteration a user would say <em>"Alexa open Artsy"</em>, then <em>"help"</em> and would get <em>"Sorry, I didn't get that artist name."</em>. Then help would say something like <em>"You can ask me about Artsy. Say cancel or stop at any time."</em> The certification process requires a prompt, eg. <em>"What artist would you like to hear about?"</em> and the session must remain open via <code>shouldEndSession(false)</code>.</p>

<a name="Skill.and.Code"></a>
<h3>Skill and Code</h3>

<p>Find the <a href="http://alexa.amazon.com/spa/index.html#skills/dp/B01MYLJO1N">Artsy skill in the Alexa app</a>.</p>

<p>Find the <a href="https://github.com/artsy/elderfield">complete Skill code on Github</a>.</p>
]]></content>
  </entry>
  
</feed>
