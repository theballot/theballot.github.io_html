<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: rspec | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/rspec/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2018-12-16T10:16:31+00:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Switch from Capybara Webkit to Chrome]]></title>
    <link href="http://artsy.github.io/blog/2018/11/27/switch-from-capybara-webkit-to-chrome/"/>
    <updated>2018-11-27T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2018/11/27/switch-from-capybara-webkit-to-chrome</id>
    <content type="html"><![CDATA[<p>Volt is the internal app name of Artsy CMS, and our partners use it to manage their inventory and presence on artsy.net. It's a Rails-based UI app that talks to many API services. We use <a href="https://github.com/rspec/rspec">RSpec</a> extensively to cover controller, model, view, and feature specs. As of Jun. 2018, Volt had 3751 specs and 495 of them were run with JavaScript enabled. It took about 16 mins to run on CircleCI with 6x parallelism.</p>

<p>Capybara-webkit was introduced from the very beginning of Volt for testing JavaScript-enabled specs via headless WebKit browser. It's been providing a lot of confidence for the past 4+ years; however, a few reasons/growing concerns have encouraged us to look for (more modern) alternatives:</p>

<!-- more -->


<a name="The.Problem"></a>
<h2>The Problem</h2>

<ul>
<li>The <a href="https://github.com/thoughtbot/capybara-webkit/tree/v1.14.0#qt-dependency-and-installation-issues">dependency of specific versions of Qt</a> has been causing frustrations to set it up properly both on engineers' local machines and on CI.</li>
<li>The roadmap of capybara-webkit development is <a href="https://github.com/thoughtbot/capybara-webkit/issues/885#issuecomment-193988527">unclear</a>.</li>
<li>It's been hard to truly identify the root cause of "flickering" feature specs (i.e. tests that fail intermittently and are hard to reliably reproduce), while retrying tended to resolve it on CI.</li>
<li>The entire RSpec tests took about 16 mins to complete on CI, with 6 parallelism. The slowness made it unrealistic to run the whole tests locally.</li>
</ul>


<a name="The.Goal"></a>
<h2>The Goal</h2>

<p>Headless Chrome has gained a lot of attention in the past years and migrations done by companies such as <a href="https://about.gitlab.com/2017/12/19/moving-to-headless-chrome/">GitLab</a> and <a href="https://robots.thoughtbot.com/headless-feature-specs-with-chrome">thoughtbot</a> have proved it to be a promising alternative to capybara-webkit. In fact, it's been <a href="http://guides.rubyonrails.org/5_1_release_notes.html#system-tests">officially included in Rails 5.1</a> for <a href="https://guides.rubyonrails.org/testing.html#system-testing">system tests</a>.</p>

<p>The goal of this project is to switch to Headless Chrome and maintain the same feature sets we have now. This includes:</p>

<ul>
<li>Making all existing specs pass</li>
<li>Running in container environments and using Artsy <a href="https://github.com/artsy/hokusai">Hokusai</a></li>
<li>Supporting mechanisms to debug specs, e.g. examining browser console logs for JavaScript behavior, taking screenshots on demand and automatically on failure, etc.</li>
<li>Bonus point to improve the stability of feature specs</li>
<li>Bonus point to improve the speed of running the entire test suite</li>
</ul>


<a name="The.How"></a>
<h2>The How</h2>

<p>First, we replaced <code>capybara-webkit</code> with <code>selenium-webdriver</code> and <code>chromedriver-helper</code>:</p>

<pre><code class="ruby">gem 'selenium-webdriver'
gem 'chromedriver-helper'
</code></pre>

<p><a href="https://github.com/flavorjones/chromedriver-helper"><code>chromedriver-helper</code></a> was useful to help install <a href="https://sites.google.com/a/chromium.org/chromedriver/">chromedriver</a> in different environments, e.g. an engineer's local machine, CI, etc.</p>

<p>Second, we registered both <code>:chrome</code> and <code>:headleass_chrome</code> drivers. By default, it used Headless Chrome as the JavaScript driver, and we could easily switch to Chrome and observe the actual interaction happening in a real browser.</p>

<pre><code class="ruby">Capybara.register_driver :chrome do |app|
  Capybara::Selenium::Driver.new(app, browser: :chrome)
end

Capybara.register_driver :headless_chrome do |app|
  caps = Selenium::WebDriver::Remote::Capabilities.chrome(loggingPrefs: { browser: 'ALL' })
  opts = Selenium::WebDriver::Chrome::Options.new

  chrome_args = %w[--headless --window-size=1920,1080 --no-sandbox --disable-dev-shm-usage]
  chrome_args.each { |arg| opts.add_argument(arg) }
  Capybara::Selenium::Driver.new(app, browser: :chrome, options: opts, desired_capabilities: caps)
end

Capybara.configure do |config|
  # change this to :chrome to observe tests in a real browser
  config.javascript_driver = :headless_chrome
end
</code></pre>

<p>We were on Rails v5.0.2 and Capybara v2.18.0 during the migration. We will be able to simplify the configuration by using the default <code>:selenium_chrome</code> and <code>:selenium_chrome_headless</code> drivers introduced in <a href="https://github.com/teamcapybara/capybara/blob/3.11.1/lib/capybara.rb#L535-L545">Capybara v3.11.1</a>. In addition, Rails v5.1 introduced the new system tests, and it'll be even simpler by using the <a href="https://api.rubyonrails.org/v5.1.3/classes/ActionDispatch/SystemTestCase.html#method-c-driven_by"><code>driven_by</code></a> method.</p>

<a name="Lessons.Learned"></a>
<h2>Lessons Learned</h2>

<p>Naively switching to Headless Chrome caused about 60 spec failures on my local machine. We simply went through them one by one and fixed them. A big part of failures was due to <a href="https://github.com/thoughtbot/capybara-webkit/tree/v1.14.0#non-standard-driver-methods">capybara-webkit's non-standard driver methods</a>, such as setting cookies, inspecting console logs, etc., and we just had to migrate to Selenium WebDriver's equivalents.</p>

<p>However, we still observed flickering specs on CI, while the exact failures seemed to be different than previously observed with Capybara Webkit. We will have to investigate farther for possible causes. Regarding speed, we didn't see significant improvement after switching to Headless Chrome, as mentioned in GitLab's and others' blog post, too.</p>

<a name="Next.Steps"></a>
<h2>Next Steps</h2>

<p>The naive migration to Chrome (and removal of the Qt dependency) already improved the developer experience quite a lot (e.g. no more wrestling with installing Capybara Webkit and Qt 5.5 on every engineer's local machine <em>and</em> CI.) There are many next steps we can keep experimenting with and improving our tests, for example</p>

<ul>
<li>Updating Volt to Rails >= 5.1 and switching to system tests</li>
<li>Investigating the causes of the flickering specs by looking into intermittent failures reported on CI</li>
<li>Improving speed by using Docker <a href="https://docs.docker.com/develop/develop-images/multistage-build/">multi-stage builds</a>, caching, writing the right type and amount of tests, etc.</li>
</ul>


<p>It's a long journey, and we were all excited about the migration and the new future. We'd love to hear your experience, too!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Splitting up a large test suite]]></title>
    <link href="http://artsy.github.io/blog/2015/09/24/splitting-up-a-large-test-suite/"/>
    <updated>2015-09-24T22:13:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/09/24/splitting-up-a-large-test-suite</id>
    <content type="html"><![CDATA[<p>A while back, we wrote about <a href="/blog/2012/10/09/how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow/">How to Run RSpec Test Suites in Parallel with Jenkins CI Build Flow</a>. A version of that still handles our largest test suite, but over time the initial division of specs became unbalanced. We ended up with some tasks that took twice as long as others. Even worse, in an attempt to rebalance task times, we ended up with awkward file patterns like <code>'spec/api/**/[a-m]*_spec.rb'</code>.</p>

<p>To keep our parallel spec tasks approximately equal in size and to support arbitrary concurrency, we've added a new <code>spec:sliced</code> task:</p>

<!-- more -->


<pre><code class="ruby">namespace :spec do
  task :set_up_spec_files do
    spec_files = Dir['spec/**/*_spec.rb']
    @spec_file_digests = Hash[spec_files.map { |f| [f, Zlib.crc32(f)] }]
  end

  RSpec::Core::RakeTask.new(:sliced, [:index, :concurrency] =&gt; :set_up_spec_files) do |t, args|
    index = args[:index].to_i
    concurrency = args[:concurrency].to_i
    t.pattern = @spec_file_digests.select { |f, d| d % concurrency == index }.keys
  end
end
</code></pre>

<p>As you can see, the <code>set_up_spec_files</code> helper task builds a hash of spec file paths and corresponding checksums. When we invoke the <code>sliced</code> task with <code>index</code> and <code>concurrency</code> values (e.g., <code>0</code> and <code>5</code>), only the spec files with checksums equal to <code>0</code> when mod-ed by <code>5</code> are run. Thus, the Jenkins build flow would look like:</p>

<pre><code class="java">parallel (
  {build("master-ci-task", tasks: "spec:sliced[0,5]")},
  {build("master-ci-task", tasks: "spec:sliced[1,5]")},
  {build("master-ci-task", tasks: "spec:sliced[2,5]")},
  {build("master-ci-task", tasks: "spec:sliced[3,5]")},
  {build("master-ci-task", tasks: "spec:sliced[4,5]")}
)
build("master-ci-succeeded")
</code></pre>

<p>Now, spec times <em>might</em> continue to be unbalanced despite files being split up approximately evenly. (For a more thorough approach based on recording spec times, see <a href="https://github.com/ArturT/knapsack">knapsack</a>.) However, this little bit of randomness was a big improvement over our previous approach, and promises to scale in a uniform manner.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Continuous integration for service-oriented architectures]]></title>
    <link href="http://artsy.github.io/blog/2014/05/12/continuous-integration-for-service-oriented-architectures/"/>
    <updated>2014-05-12T10:50:00+00:00</updated>
    <id>http://artsy.github.io/blog/2014/05/12/continuous-integration-for-service-oriented-architectures</id>
    <content type="html"><![CDATA[<p>Whatever you have against monolithic architectures, at least they're easy to test. And when those tests succeed, you can be reasonably confident the live app will work the same way.</p>

<p>Artsy began as one such monolithic app, but we've been refactoring into an ecosystem of related APIs and sites. Today, when you search for <a href="https://artsy.net/gene/cultural-commentary">"cultural commentary"</a> or visit <a href="https://artsy.net/artist/robert-longo">Robert Longo</a> on <a href="https://artsy.net">artsy.net</a>, the page is rendered by a web app, sources data from an API, retrieves recommendations from a separate service, tracks trends in another, and records analytics in yet another.</p>

<p>This was a boost for developer productivity and scaling, but eviscerated the value of our tests. We repeatedly encountered bugs that were failings of <em>the interaction between codebases</em> rather than failings of individual ones. Test libraries and tools typically concern themselves with one isolated app. When you have services that consume services that consume services, those isolated tests (with their stubs of everything else) don't necessarily reflect production's reality.</p>

<p>So how should we develop our small, focused apps (or <a href="http://en.wikipedia.org/wiki/Service-oriented_architecture">service-oriented architecture</a>, or <a href="http://martinfowler.com/articles/microservices.html">microservices</a>...) with confidence? We set out to build a dedicated acceptance test suite that would run tests across multiple services, configuring and integrating them in a way that closely matches the production environment.</p>

<!-- more -->


<a name="The.code"></a>
<h2>The code</h2>

<p>We'll take the simplest example possible of 2 related applications: a trivial Ruby API serving a Node.js-based web app. (You can also go directly to <a href="https://github.com/joeyAghion/multiapp_example-tests">the source</a>.)</p>

<p><a href="http://david.heinemeierhansson.com/2014/tdd-is-dead-long-live-testing.html">Recent</a> <a href="http://blog.8thlight.com/uncle-bob/2014/04/25/MonogamousTDD.html">debates</a> <a href="https://news.ycombinator.com/item?id=7659251">aside</a>, I like to start with a test:</p>

<pre><code class="ruby">feature "home", js: true do

  scenario "welcomes visitor" do
    visit "/"
    expect(page).to have_content("Browse products")
  end
end
</code></pre>

<p>We're using the popular [and familiar] <a href="https://github.com/jnicklas/capybara">Capybara</a> with <a href="https://relishapp.com/rspec">RSpec</a> and <a href="http://docs.seleniumhq.org/">Selenium</a>. Naturally, our test fails right away:</p>

<pre><code class="bash">$ bundle exec rspec
# ...
     Failure/Error: visit "/"
     Selenium::WebDriver::Error::UnknownError:
       Target URL / is not well-formed.
</code></pre>

<p>There are a few steps to getting our projects installed and running as part of the test suite. First, we'll add git submodules in the <code>/api</code> and <code>/web</code> subdirectories that <a href="http://stackoverflow.com/questions/9189575/git-submodule-tracking-latest">track the master branch</a> of each project.</p>

<pre><code class="bash">git submodule add -b master git@github.com:joeyAghion/multiapp_example-api.git api
git submodule add -b master git@github.com:joeyAghion/multiapp_example-web.git web
</code></pre>

<p>Next, create Rake tasks to install prerequisites for each project.</p>

<pre><code class="ruby"># Rakefile
require 'childprocess'
require 'rspec/core/rake_task'

RSpec::Core::RakeTask.new(:spec)

task :ci =&gt; ['checkout', 'install', 'spec']

task :checkout do
  sh %{git submodule update --remote --init} do |ok, res|
    raise "Submodule update failed with status #{res.exitstatus}" unless ok
  end
end

task :install =&gt; ['api:install', 'web:install']

namespace :api do
  task :install do
    Bundler.with_clean_env do
      proc = ChildProcess.build('bundle', 'install')
      proc.io.inherit!
      proc.cwd = './api'
      proc.start
      proc.wait
      raise "bundle install exited with status #{proc.exit_code}" unless proc.exit_code == 0
    end
  end
end

namespace :web do
  task :install do
    proc = ChildProcess.build('npm', 'install')
    proc.io.inherit!
    proc.cwd = './web'
    proc.start
    proc.wait
    raise "npm install existed with status #{proc.exit_code}" unless proc.exit_code == 0
  end
end
</code></pre>

<p>The new <code>checkout</code> and <code>install</code> tasks make sure we have the latest code and all prerequisites installed. Note how we use <code>Bundler.with_clean_env</code> to isolate the API (which has its own Gemfile and bundler environment) from the test suite.</p>

<p>Now that the API and web apps are set up, we'll use RSpec's <code>before(:suite)</code> and <code>after(:suite)</code> hooks to start and stop them around each test run.</p>

<pre><code class="ruby"># spec/spec_helper.rb
require 'capybara/rspec'
require 'childprocess'

API_PORT = 7000
WEB_PORT = 7001

Capybara.configure do |config|
  config.current_driver = :selenium
  config.run_server = false
  config.app_host = "http://localhost:#{WEB_PORT}"
end

RSpec.configure do |config|
  # ...
  config.before(:suite) do
    start_api
    start_web
  end

  config.after(:suite) do
    stop_api
    stop_web
  end
end

def start_api
  $stderr.puts "Starting API..."
  Bundler.with_clean_env do
    $api = ChildProcess.build('bundle', 'exec', 'ruby', 'app.rb')
    $api.cwd = './api'
    $api.io.inherit!
    $api.environment['PORT'] = API_PORT
    $api.start
    $stderr.puts "Waiting for API to start listening..."
    sleep(1) while !listening_on?(API_PORT) &amp;&amp; $api.alive?
  end
end

def stop_api
  $stderr.puts "Stopping API..."
  $api.stop
end

def start_web
  $stderr.puts "Starting web..."
  $web = ChildProcess.build('node', 'app.js')
  $web.cwd = './web'
  $web.io.inherit!
  $web.environment['API_URL'] = "http://localhost:#{API_PORT}"
  $web.environment['PORT'] = WEB_PORT
  $web.start
  $stderr.puts "Waiting for web to start listening..."
  sleep(1) while !listening_on?(WEB_PORT) &amp;&amp; $web.alive?
end

def stop_web
  $stderr.puts "Stopping web..."
  $web.stop
end

def listening_on?(port)
  system("netstat -an | grep #{port} | grep LISTEN")
end
</code></pre>

<p>Running <code>rake spec</code> now starts up and waits for both apps, runs our test, and...</p>

<pre><code>Starting API...
Waiting for API to start listening...
# ...
Starting web...
Waiting for web to start listening...
# ...
home
  welcomes visitor
Stopping API...
# ...
Stopping web...

Finished in 4.67 seconds
1 example, 0 failures
</code></pre>

<p>Success!</p>

<p>Well, sort of. Our test of the home page doesn't even depend on both systems. Let's try a more meaningful test, listing products from the API.</p>

<pre><code class="ruby">feature "shop", js: true do

  scenario "list widgets" do
    visit "/"
    click_link "Browse products"
    expect(page).to have_content("Foo Widget")
  end
end
</code></pre>

<p>Will it work?</p>

<pre><code class="bash">Failures:

  1) shop list widgets
     Failure/Error: expect(page).to have_content("Foo Widget")
       expected to find text "Foo Widget" in ""
     # ./spec/shop_spec.rb:8:in `block (2 levels) in &lt;top (required)&gt;'
</code></pre>

<p>The web app isn't authenticated to use the API! This brings up a more general question:</p>

<a name="How.to.bootstrap.test.data"></a>
<h2>How to bootstrap test data</h2>

<p>Most testing frameworks offer fixtures or direct access to the database. Because the API runs in a separate process, things are a little more difficult. We opt for 1 of 2 approaches, depending on the context:</p>

<ul>
<li><strong>Insert data directly into the API's database.</strong> We tend to do this only as a last resort, because tests would presume knowledge of the API's implementation.</li>
<li><strong>Perform test set-up via the API.</strong> Slightly nicer, and closer to real-life clients. (However, the API must be fairly complete.)</li>
</ul>


<p>In practice, we "cheat" and use direct database-insertion to initially bootstrap an API client application, then perform further test set-up through the API. You should choose what's most convenient.</p>

<p>Our simple example will register the web application as an API client, then pass a key via basic authentication. We'll have to modify the <code>start_web</code> helper:</p>

<pre><code class="ruby">def start_web
  $stderr.puts "Starting web..."
  $web = ChildProcess.build('node', 'app.js')
  $web.cwd = './web'
  $web.io.inherit!
  $api_base_url = "http://#{api_client['key']}:@localhost:#{API_PORT}"
  $web.environment['API_URL'] = $api_base_url
  $web.environment['PORT'] = WEB_PORT
  $web.start
  $stderr.puts "Waiting for web to start listening..."
  sleep(1) while !listening_on?(WEB_PORT) &amp;&amp; $web.alive?
end

def api_client
  $api_client ||= begin
    response = Net::HTTP.post_form(URI("http://localhost:#{API_PORT}/api/clients"), {})
    JSON.parse(response.body)
  end
end
</code></pre>

<p>And the test will need to set up the data it expects to find listed:</p>

<pre><code class="ruby">feature "shop", js: true do

  scenario "list widgets" do
    create_widget(name: 'Foo Widget', price_cents: 100_00)
    visit "/"
    click_link "Browse products"
    expect(page).to have_content("Foo Widget")
  end
end

# spec/spec_helper.rb
def create_widget(params = {})
  Net::HTTP.post_form(URI("#{$api_base_url}/api/widgets"), params)
end
</code></pre>

<p>Lo and behold, our entire "suite" now passes:</p>

<pre><code class="bash">2 examples, 0 failures
</code></pre>

<p>This basic structure has accommodated dozens of test scenarios. We've extended it with database- and cache-clearing between tests, and organized helpers into modules under <code>spec/support</code>. The suite is built nightly against the latest versions of our codebases, and has caught a few significant bugs.</p>

<p>A caveat: with so many layers and dependencies involved, there are often spurious failures. We've picked up a few practices that help:</p>

<ul>
<li><a href="http://artsy.github.io/blog/2012/05/15/how-to-organize-over-3000-rspec-specs-and-retry-test-failures/">Automatic retries</a></li>
<li><a href="http://artsy.github.io/blog/2014/01/30/isolating-spurious-and-nondeterministic-tests/">Quarantine for problematic tests</a></li>
<li><a href="https://github.com/mattheworiordan/capybara-screenshot">Failure screenshots</a></li>
</ul>


<p>You can <a href="https://github.com/joeyAghion/multiapp_example-tests">grab the example code</a>. And make sure to let us know in the comments how <em>you</em> approach testing across applications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Isolating Spurious and Nondeterministic Tests]]></title>
    <link href="http://artsy.github.io/blog/2014/01/30/isolating-spurious-and-nondeterministic-tests/"/>
    <updated>2014-01-30T14:42:00+00:00</updated>
    <id>http://artsy.github.io/blog/2014/01/30/isolating-spurious-and-nondeterministic-tests</id>
    <content type="html"><![CDATA[<p>Testing is a critical part of our workflow at <a href="https://artsy.net">Artsy</a>. It gives us confidence to make regular, aggressive enhancements. But anyone who has worked with a large, complex test suite has struggled with occasional failures that are difficult to reproduce or fix.</p>

<p>These failures might be due to slight timing differences or lack of proper isolation between tests. Integration tests are particularly thorny, since problems can originate not only in application code, but in the browser, testing tools (e.g., <a href="http://docs.seleniumhq.org/">Selenium</a>), database, network, or external APIs and dependencies.</p>

<a name="The.Quarantine"></a>
<h2>The Quarantine</h2>

<p>We've been <a href="http://artsy.github.io/blog/2012/05/15/how-to-organize-over-3000-rspec-specs-and-retry-test-failures/">automatically retrying failed tests</a>, with some success. However, these problems tend to get worse. (If you have 10 tests that each have a 1% chance of failing, roughly 1 in 10 builds will fail. If you have 50, 4 in 10 builds will fail.)</p>

<p>Martin Fowler offers the most compelling thoughts on this topic in <a href="http://martinfowler.com/articles/nonDeterminism.html">Eradicating Non-Determinism in Tests</a>. (Read it, really.) He suggests quarantining problematic tests in a separate suite, so they don't block the build pipeline.</p>

<!-- more -->


<a name="Setting.it.up"></a>
<h2>Setting it up</h2>

<p>This turned out to be pretty easy to set up, using our preferred tools of <a href="https://relishapp.com/rspec">RSpec</a> and <a href="http://travis-ci.com/">Travis</a>. First, tag a problem test with <code>spurious</code>:</p>

<pre><code>it 'performs tricky browser interaction', spurious: true do
  ...
end
</code></pre>

<p>Your continuous integration script can exclude the tagged tests as follows:</p>

<pre><code>bundle exec rspec --tag ~spurious
</code></pre>

<p>We'd like to be aware of spurious failures, but not allow them to fail the build. In our app's <code>.travis.yml</code> file, this is as simple as adding a script entry that always exits with <code>0</code> status:</p>

<pre><code>language: ruby
rvm:
  - 1.9.3
script:
  - "bundle exec rspec --tag ~spurious"
  - "bundle exec rspec --tag spurious || true"
</code></pre>

<p>We'll see any spurious failures in the build's output, but our pipeline won't be affected.</p>

<a name="Bonus:.Limiting.quarantined.tests"></a>
<h2>Bonus: Limiting quarantined tests</h2>

<p>So, what prevents the quarantine from getting larger and larger, while the test suite gets weaker and weaker? Fowler <a href="http://martinfowler.com/articles/nonDeterminism.html#Quarantine">recommends</a> enforcing a limit on the number of quarantined tests (e.g., 8).</p>

<p>We can even trigger a build failure if the limit is exceeded. This <code>.travis.yml</code> writes the spurious suite's abbreviated output to a file, then asserts that the summary mentions no more than "8 examples":</p>

<pre><code>language: ruby
rvm:
  - 1.9.3
script:
  - "bundle exec rspec --tag ~spurious"
  - "bundle exec rspec --tag spurious --format documentation --format progress --out spurious.out || true"
  - "[[ $(grep -oE '^\d+' spurious.out) -le 8 ]]"
</code></pre>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>The quarantine is no excuse to create tests that fail under realistic conditions. It's simply a framework for recognizing and, eventually, fixing or eliminating the problematic tests that inevitably crop up in a complex environment.</p>

<p>Hopefully, our experiment is useful to other teams struggling with unreliable builds. Share any feedback in the comments!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Run RSpec Test Suites in Parallel with JenkinsCI Build Flow]]></title>
    <link href="http://artsy.github.io/blog/2012/10/09/how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow/"/>
    <updated>2012-10-09T21:21:00+00:00</updated>
    <id>http://artsy.github.io/blog/2012/10/09/how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow</id>
    <content type="html"><![CDATA[<p>We now have over 4700 RSpec examples in one of our projects. They are stable, using the techniques described in an <a href="/blog/2012/02/03/reliably-testing-asynchronous-ui-w-slash-rspec-and-capybara/">earlier post</a> and organized in <a href="/blog/2012/05/15/how-to-organize-over-3000-rspec-specs-and-retry-test-failures/">suites</a>. But they now take almost 3 hours to run, which is clearly unacceptable.</p>

<p>To solve this, we have parallelized parts of the process with existing tools, and can turn a build around in just under an hour. This post will dive into our <a href="http://jenkins-ci.org/">Jenkins</a> build flow setup.</p>

<p>To keep things simple, we're going to only build the <code>master</code> branch. When a change is committed on <code>master</code> we're going to push <code>master</code> to a <code>master-ci</code> branch and trigger a distributed build on <code>master-ci</code>. Once all the parts have finished, we'll complete the build by pushing <code>master-ci</code> to <code>master-succeeded</code> and notify the dev team of success or failure.</p>

<p>Here's a diagram of what's going on.</p>

<p><img src="/images/2012-10-09-how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow/master-ci.png"></p>

<!-- more -->


<a name="Plugins"></a>
<h2>Plugins</h2>

<p>Install the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Build+Flow+Plugin">Build Flow</a> and the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Parameterized+Trigger+Plugin">Parameterized Trigger</a> plugin. Grant <code>Anonymous</code> job read permissions in Jenkins system configuration (see <a href="https://issues.jenkins-ci.org/browse/JENKINS-14027">JENKINS-14027</a>).</p>

<p>Create the following Jenkins jobs.</p>

<a name="master-prequel"></a>
<h2>master-prequel</h2>

<p>A free-style job that connects to the SCM, in our case Git.</p>

<ul>
<li>Set SCM repository URL to your Git repo, eg. <code>git@github.com:spline/reticulator.git</code></li>
<li>Change the default branch specifier from <code>**</code> to <code>master</code>. We'll be pushing a <code>master-ci</code> branch, which could, in turn, cause more builds if you don't do this.</li>
<li>Add a post-build action to build another project. Trigger the <code>master</code> project if the build succeeds.</li>
</ul>


<a name="master"></a>
<h2>master</h2>

<p>This is a build-flow job. We'll describe the individual tasks that the flow invokes further. The flow DSL looks as follows.</p>

<pre><code class="ruby">build("master-ci-init")
parallel (
 { build("master-ci-task", tasks: "spec:suite:models:ci") },
 { build("master-ci-task", tasks: "spec:suite:api:ci") },
 { build("master-ci-task", tasks: "spec:suite:integration:ci") }
)
build("master-ci-succeeded")
</code></pre>

<p>This is a good place to add an e-mail notification post-build action for every unstable build.</p>

<a name="master-ci-init"></a>
<h2>master-ci-init</h2>

<p>A free-style job that creates the <code>master-ci</code> branch from master. It needs to be connected to your SCM and executes the following shell script.</p>

<pre><code class="bash">#!/bin/bash
git checkout $GIT_BRANCH
git push origin -f $GIT_BRANCH:$GIT_BRANCH\-ci
</code></pre>

<p>Note that we cannot combine this task with <code>master-prequel</code>, because we have to make sure the branch creation runs once under the entire flow, while <code>master-prequel</code> can be run multiple times, once per check-in. Otherwise the <code>master-ci</code> branch could get updated before a <code>master-ci-task</code> runs from a previous flow execution.</p>

<a name="master-ci-task"></a>
<h2>master-ci-task</h2>

<p>A parameterized build that accepts a <code>tasks</code> parameter that the flow will pass in.</p>

<p>Change the default branch specifier to <code>master-ci</code> and execute the following shell script.</p>

<pre><code class="bash">#!/bin/bash
bundle install
bundle exec rake $tasks
</code></pre>

<p>This example runs <code>rake $tasks</code>, which we define to be various test suites in our flow DSL. Our test suite setup is described in <a href="/blog/2012/05/15/how-to-organize-over-3000-rspec-specs-and-retry-test-failures/">this post</a>. Your mileage may vary.</p>

<a name="master-ci-succeeded"></a>
<h2>master-ci-succeeded</h2>

<p>This is an optional step. We use this free-style job to tag <code>master-ci</code> as <code>master-succeeded</code> with the following shell script.</p>

<pre><code class="bash">#!/bin/bash
git checkout $GIT_BRANCH
git push origin -f $GIT_BRANCH:${GIT_BRANCH/%-ci/}-succeeded
</code></pre>

<p>Our deployment to production will pickup the <code>master-succeeded</code> branch when it's time.</p>

<a name="Improvements."></a>
<h2>Improvements?</h2>

<p>I see a few possible improvements here that might require a bit of work.</p>

<ul>
<li>The ability to split an RSpec suite up across an arbitrary number N sub-jobs and M executors would create an optimal parallel split based on the resources available.</li>
<li>Passing the value of <code>GIT_BRANCH</code> and <code>GIT_COMMIT</code> across these jobs would enable building any branch and eliminate the need for <code>master-ci-init</code>.</li>
<li>Build flow could support SCM polling the same way as free-style jobs, avoiding the need for <code>master-prequel</code>. We weren't able to get a stable notification of changes from Github with the Jenkins Github plugin.</li>
</ul>


<p>Please suggest further improvements in the comments below!</p>

<p>(Update: See <a href="/blog/2015/09/24/splitting-up-a-large-test-suite/">Splitting up a large test suite</a> for a modified approach that splits work approximately evenly among an arbitrary number of sub-jobs.)</p>
]]></content>
  </entry>
  
</feed>
