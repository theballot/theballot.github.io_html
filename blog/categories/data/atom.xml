<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: data | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/data/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2018-12-16T10:16:31+00:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Programmer Misconceptions about Art]]></title>
    <link href="http://artsy.github.io/blog/2018/04/18/programmer-misconceptions-about-art/"/>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2018/04/18/programmer-misconceptions-about-art</id>
    <content type="html"><![CDATA[<p>Our mission at Artsy has been to make a world where everyone is moved by art every day, and at a high level, the way that our engineering team supports that mission is through building software. We have built systems and databases and user interfaces that represent different facets of the art world, and throughout our work, we have... made some mistakes.</p>

<p>That's okay! Programmers make mistakes all the time. There is <a href="https://github.com/kdeldycke/awesome-falsehood">a large list of blog posts</a> describing various programmer misconceptions, from subjects you might expect would be simple to model in computers, like units of measurement and time, to subjects that are based more in the human condition, like postal addresses and marriage.</p>

<p>In the interest of openness and sharing what we've learned, the Artsy Engineering team has come up with the following list of misconceptions programmers believe about art. Thank you to everyone at Artsy who contributed to this list.</p>

<!-- more -->


<ul>
<li>All artworks have an artist (some artworks are attributed to "cultural makers", others have a manufacturer).</li>
<li>All artworks have exactly one artist (some artworks are collaborations).</li>
<li>All artworks are unique (there are editions, reproductions, and series of works, and modeling the relationships between them all is nontrivial).</li>
<li>All lots in an art auction are artworks (some lots are "experiential", like a visit to an artist's studio).</li>
<li>Only rich people buy art.</li>
<li>Only rich people can afford to buy art, and everyone else just buys posters of "real" art.</li>
<li>All artworks have a title (some are untitled).</li>
<li>"Untitled" signifies an artwork has no title (some artworks are titled "Untitled").</li>
<li>All artwork titles can fit inside 512 characters (not true, <a href="https://www.artsy.net/artwork/matt-goerzen-sockpuppet-theatre-representing-the-techniques-tools-and-environments-whereby-hackers-and-other-info-warriors-might-seek-to-parse-through-elsewhere-distorted-informational-domains-to-make-sense-of-them-and-also-possibly-to-acquire-by-illicit-or-clever-means-good-information-that-can-then-be-communicated-in-a-way-that-sheds-light-on-deceptions-but-can-also-be-difficult-to-evaluate-on-their-own-terms-due-to-the-elite-requisites-of-interpreting-such-knowledge-or-more-generalized-uncertaintities-regarding">here is a counterexample</a>).</li>
<li>An artwork is associated with a natural, canonical category.</li>
<li>An artwork belongs to only one gallery/collector/auction house at a time (provenance of artworks is complicated, and there is no canonical source of truth).</li>
<li>Art should always be rendered at its maximum size (there are complex business constraints and art world norms that need to be considered).</li>
<li>People buy art mostly for its visual qualities (most people buy art because of a story, because they understand what the artwork is trying to say, or because they simply can't stop thinking about it).</li>
<li>People don't buy art from JPEGs (in fact, people buy art that hasn't even been created yet).</li>
<li>"My kid can paint that" (<a href="https://twitter.com/ashfurrow/status/707273704640798720">but did they?</a>).</li>
<li>The art market needs technology because it's inefficient (the art market needs technology because technology can help expand the entire art world).</li>
<li>Intermediaries in the art market are bad (eg. galleries: they enable artists to make works for years before they sell anything, they are the enabler, not the obstacle).</li>
<li>There is one "art world" (there are thousands of galleries around the world, specializing in everything from contemporary jewelry and emerging conceptual art to Chinese scroll painting and regional landscapes).</li>
<li>Your opinion on art doesn't matter, the industry will independently determine value of an artwork (everyone has opinions, your appreciation of art is <em>all</em> about <em>you</em>).</li>
<li>The art world is hermetic and isn't relevant to my life (in fact the arts contribute billions of dollars to the economy, employ thousands of people, have a ripple effect on urban life, and are often a major source of inspiration for the TV, movies, and books we all consume on a daily basis).</li>
<li>Gallerists are fancy people in a luxury business, living fancy lives (in fact, the average salary for a gallery owner is way lower than you think).</li>
<li>Art and engineering are orthogonal (nope, just look at us!).</li>
</ul>


<p>Do you have expertise in an area programmers often get wrong? Write a blog post and add it to <a href="https://github.com/kdeldycke/awesome-falsehood">the list of misconceptions</a>!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Detecting trends using Forgetsy]]></title>
    <link href="http://artsy.github.io/blog/2014/03/17/detecting-trends-with-forgetsy/"/>
    <updated>2014-03-17T11:32:00+00:00</updated>
    <id>http://artsy.github.io/blog/2014/03/17/detecting-trends-with-forgetsy</id>
    <content type="html"><![CDATA[<p><img src="/images/2014-03-17-detecting-trends-with-forgetsy/monolith.jpg" alt="Armory Trending Screen" /></p>

<p>As part of our partnership with <a href="https://www.thearmoryshow.com/">The New York Armory Show</a> this year, we installed a number of terminals throughout the fair. These screens used our own real-time data to display an ever shifting set of trending artworks, artists, and booths, to the attendees.</p>

<p>Out of this work, we've open-sourced <a href="https://github.com/cavvia/forgetsy">Forgetsy</a>, a lightweight Ruby trending library. Put simply, Forgetsy implements data structures that forget. Loosely based on Bit.ly's <a href="http://word.bitly.com/post/41284219720/forget-table">Forget Table</a> concept, Forgetsy uses decaying counters to track temporal trends in categorical distributions.</p>

<!-- more -->


<a name="Anatomy.of.a.Trend"></a>
<h2>Anatomy of a Trend</h2>

<p>To clarify the term 'trend', let's take this graph of cumulative artist searches over time as an example.</p>

<p><img src="/images/2014-03-17-detecting-trends-with-forgetsy/searches.png" alt="Artist Search Graphs" /></p>

<p>On the left-hand side, we see a steepening gradient (denoted by the dashed lines) for Banksy during his residency in New York (Oct 2013), but in contrast a linear rise in searches for Warhol over the same period. We define a 'trend' as this rise in the <em>rate</em> of observations of a particular event over a time period, which we'll call τ.</p>

<p>In Forgetsy, trends are encapsulated by a construct named <em>delta</em>. A <em>delta</em> consists of two sets of counters, each of which implements <a href="https://en.wikipedia.org/wiki/Exponential_decay">exponential decay</a> of the following form.</p>

<p><img src="http://latex.codecogs.com/gif.latex?X_t_1%3DX_t_0%5Ctimes%7Be%5E%7B-%5Clambda%5Ctimes%7Bt%7D%7D%7D" alt="Exponential Decay" /></p>

<p>Where the inverse of the decay rate (λ) is the lifetime of an observation in the set, τ. By normalising one set by a set with half the decay rate (or double the lifetime), we obtain a trending score for each category in a distribution. This score expresses the change in the rate of observations of a category over the lifetime of the set, as a proportion in the range [0,1].</p>

<p>Forgetsy removes the need for manually sliding time windows or explicitly maintaining rolling counts, as observations naturally decay away over time. It's designed for heavy writes and sparse reads, as it implements decay at read time. Each set is implemented as a <a href="http://redis.io/">redis</a> sorted set, and keys are scrubbed when a count is decayed to near zero, providing storage efficiency.</p>

<p>As a result, Forgetsy handles distributions with up to around 10<sup>6</sup> active categories, receiving hundreds of writes per second, without much fuss.</p>

<a name="Usage"></a>
<h2>Usage</h2>

<p>Take a social network in which users can follow each other. You want to track trending users. You construct a delta with a one week lifetime, to capture trends in your follows data over one week periods:</p>

<pre><code class="ruby">follows_delta = Forgetsy::Delta.create('user_follows', t: 1.week)
</code></pre>

<p>The delta consists of two sets of counters indexed by category identifiers. In this example, the identifiers will be user ids. One set decays over the mean lifetime specified by τ, and another set decays over double the lifetime.</p>

<p>You can now add observations to the delta, in the form of follow events. Each time a user follows another, you increment the followed user id. You can also do this retrospectively:</p>

<pre><code class="ruby">follows_delta.incr('UserFoo', date: 2.weeks.ago)
follows_delta.incr('UserBar', date: 10.days.ago)
follows_delta.incr('UserBar', date: 1.week.ago)
...
</code></pre>

<p>Providing an explicit date is useful if you are processing data asynchronously. You can also use the <code>incr_by</code> option to increment a counter in batches. You can now consult your follows delta to find your top trending users:</p>

<pre><code class="ruby">puts follows_delta.fetch
</code></pre>

<p>Will print:</p>

<pre><code class="ruby">{ 'UserFoo' =&gt; 0.789, 'UserBar' =&gt; 0.367 }
</code></pre>

<p>Each user is given a dimensionless score in the range [0,1] corresponding to the normalised follows delta over the time period. This expresses the proportion of follows gained by the user over the last week compared to double that lifetime.</p>

<p>Optionally fetch the top <em>n</em> users, or an individual user's trending score:</p>

<pre><code class="ruby">follows_delta.fetch(n: 20)
follows_delta.fetch(bin: 'UserFoo')
</code></pre>

<p>For more information on usage, check out the <a href="https://github.com/cavvia/forgetsy">github project</a> page.</p>

<a name="In.the.Wild"></a>
<h2>In the Wild</h2>

<p>In practice, we use linear, weighted combinations of deltas to produce trending scores for any given domain, such as artists. Forgetsy doesn't provide a server, but we send events to an rpc service that updates the deltas in a streamed manner. These events might include artist follows, artwork favorites, auction lot sales or individual page views.</p>

<p>One requirement we have is lifetime flexibility. Forgetsy lets us stipulate the trending period τ on a delta by delta basis. This allows us to lower the lifetime significantly in a fair context, in which we track trends over just a few hours, contrasted with a general art market context, in which we're interested in trends over weeks and months.</p>

<p>In summary, the delta structures provided by Forgetsy provide you with a simple, scalable, transparent base for a trending algorithm that can be tuned to suit the specific dynamics of the domain in question.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Start Small with Big Data and Google Analytics]]></title>
    <link href="http://artsy.github.io/blog/2012/05/01/how-to-start-small-with-big-data-and-google-analytics/"/>
    <updated>2012-05-01T20:52:00+00:00</updated>
    <id>http://artsy.github.io/blog/2012/05/01/how-to-start-small-with-big-data-and-google-analytics</id>
    <content type="html"><![CDATA[<p>Why do so many companies write a homegrown pageviews tracking system? Between Google Analytics, Kissmetrics and many others, isn't that a completely solved problem?</p>

<p>These popular solutions lack domain knowledge. They are easily capable of segmenting users by region or browser, but they fail to recognize rules core to your business. Tracking pageviews with a homegrown system becomes your next sprint's goal.</p>

<p>Implementing a hit counter service is quite tricky. This is a write-heavy, asynchronous problem that must minimize impact on page rendering time, while dealing with rapidly growing amounts of data. Is there a middle ground between using Google Analytics and rolling out our own homegrown implementation? How can we use Google Analytics for data collection and inject domain knowledge into gathered data, incrementally, without writing our own service?</p>

<!--more-->


<p>Let's write a Rake task that pulls data from Google Analytics. We can run it daily. Start with a Ruby gem called <a href="https://github.com/vigetlabs/garb">Garb</a>.</p>

<pre><code class="ruby">gem "garb", "0.9.1"
</code></pre>

<p>Garb requires Google Analytics credentials. Those can go into a YAML configuration file, which will use environment settings in production (it's an ERB template, too). We can hardcode the test account values.</p>

<pre><code class="yaml config/google_analytics.yml">defaults: &amp;defaults

development, test:
  &lt;&lt;: *defaults
  email: "ga@example.com"
  password: "password"
  ua: "UA-12345678-1"

production:
  &lt;&lt;: *defaults
  email: &lt;%= ENV['GOOGLE_ANALYTICS_EMAIL'] %&gt;
  password: &lt;%= ENV['GOOGLE_ANALYTICS_PASSWORD'] %&gt;
  ua: &lt;%= ENV['GOOGLE_ANALYICS_UA'] %&gt;
</code></pre>

<p>Establish a Google Analytics session and fetch the profile corresponding to the Google user account with Garb.</p>

<pre><code class="ruby">config = YAML.load(ERB.new(File.new(Rails.root.join("config/google_analytics.yml")).read).result)[Rails.env].symbolize_keys
Garb::Session.login(config[:email], config[:password])
profile = Garb::Management::Profile.all.detect { |p| p.web_property_id == config[:ua] }
raise "missing profile #{config[:ua]} in #{Garb::Management::Profile.all.map(&amp;:web_property_id)}" unless profile
</code></pre>

<p>Garbs needs a data model to collect pageviews. It extends <code>Garb::Model</code> and defines a set of "metrics" and "dimensions".</p>

<pre><code class="ruby app/models/google_analytics_pageviews.rb">class GoogleAnalyticsPageviews
  extend Garb::Model
  metrics :pageviews
  dimensions :page_path
end
</code></pre>

<p>You can play with the <a href="http://ga-dev-tools.appspot.com/explorer/">Google Analytics Query Explorer</a> to see the many possible metrics (such as pageviews) and dimensions (such as requested page path).</p>

<p>By default, Google Analytics lets clients retrieve 1000 records in a single request. To get all records we can add an iterator, called <code>all</code>, that will keep making requests until the server runs out of data. The code for <em>config/initializers/garb_model.rb</em> is <a href="https://gist.github.com/2265877">in this gist</a> and I made a <a href="https://github.com/vigetlabs/garb/pull/116">pull request</a> into Garb if you'd rather merge that onto your fork.</p>

<p>The majority of our pages are in the form of "/model/id", for example "/artwork/leonardo-mona-lisa". We're interested in all pageviews for a given artwork and in pageviews for a given artist, at a given date. We'll store selected Google Analytics data in a <code>GoogleAnalyticsPageviewsRecord</code> model described further.</p>

<pre><code class="ruby">t = Date.today - 1
GoogleAnalyticsPageviews.all(profile, { :start_date =&gt; t, :end_date =&gt; t }) do |row|
  model = /^\/#\!\/(?&lt;type&gt;[a-z-]+)\/(?&lt;id&gt;[a-z-]+)$/.match(row.page_path)
  next unless (model[:type] == "artwork" || model[:type] == "artist")
  GoogleAnalyticsPageviewsRecord.create!({
    :model_type =&gt; model[:type],
    :model_id =&gt; model[:id],
    :pageviews =&gt; row.pageviews,
    :dt =&gt; t.strftime("%Y-%m-%d")
  })
end
</code></pre>

<p>Each <code>GoogleAnalyticsPageviewsRecord</code> contains the total pageviews for a given model ID at a given date. We now have a record for each artwork and artist. We can rollup existing data into a set of collections, incrementally. For example, <code>google_analytics_artworks_monthly</code> will contain the monthly hits for each artwork.</p>

<pre><code class="ruby">class GoogleAnalyticsPageviewsRecord
  include Mongoid::Document

  field :model_type, type: String
  field :model_id, type: String
  field :pageviews, type: Integer
  field :dt, type: Date

  index [
    [:model_type, Mongo::ASCENDING],
    [:model_id, Mongo::ASCENDING],
    [:dt, Mongo::DESCENDING]
  ], :unique =&gt; true

  after_create :rollup

  def rollup
    Mongoid.master.collection("google_analytics_#{self.model_type}s_total").update(
      { :model_id =&gt; self.model_id },
      { "$inc" =&gt; { "count" =&gt; self.pageviews }}, { :upsert =&gt; true })
    {
      :daily =&gt; self.dt.strftime("%Y-%m-%d"),
      :weekly =&gt; self.dt.beginning_of_week.strftime("%Y-%W"),
      :monthly =&gt; self.dt.beginning_of_month.strftime("%Y-%m"),
      :yearly =&gt; self.dt.beginning_of_year.strftime("%Y")
    }.each_pair do |t, dt|
      Mongoid.master.collection("google_analytics_#{self.model_type}s_#{t}").update(
        { :model_id =&gt; self.model_id, :dt =&gt; dt },
        { "$inc" =&gt; { "count" =&gt; self.pageviews }}, { :upsert =&gt; true })
    end
  end

end
</code></pre>

<p>The rollup lets us query these tables directly. For example, the following query returns a record with the pageviews for the Leonardo's "Mona Lisa" in January 2012.</p>

<pre><code class="ruby">Mongoid.master.collection("google_analytics_artworks_monthly").find_one({
  :model_type =&gt; "artwork", :model_id =&gt; "leonardo-mona-lisa", :dt =&gt; "2012/01"
})
</code></pre>

<p>One of the obvious advantages of pulling Google Analytics data is the low volume of requests and offline processing. We're letting Google Analytics do the hard work of collecting data for us in real time and are consuming its API without the performance or time pressures.</p>
]]></content>
  </entry>
  
</feed>
